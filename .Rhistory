colnames(gtmp_coords) <- c('x','y')
# Treat outliers
bpx <- boxplot(gtmp_coords$x, plot = FALSE)
gtmp_coords$x[gtmp_coords$x > bpx$stats[5,1]] <- bpx$stats[5,1]
gtmp_coords$x[gtmp_coords$x < bpx$stats[1,1]] <- bpx$stats[1,1]
bpy <- boxplot(gtmp_coords$y, plot = FALSE)
gtmp_coords$y[gtmp_coords$y > bpy$stats[5,1]] <- bpy$stats[5,1]
gtmp_coords$y[gtmp_coords$y < bpy$stats[1,1]] <- bpy$stats[1,1]
gtmp_coords_df <- data.frame('node' = V(gtmp)$name,
'x' = scale(gtmp_coords[,1]) +
coords_all_centers$mean_x[coords_all_centers$X_C == i] * 1.5,
'y' = scale(gtmp_coords[,2]) +
coords_all_centers$mean_y[coords_all_centers$X_C == i] * 1.5)
}) %>% rbind.fill()
coords_special2 <- coords_special[match(V(g1)$name, coords_special$node),]
all(coords_special2$node == V(g1)$name)
coords_special2 <- coords_special2[,c('x','y')]
rownames(coords_special2) <- NULL
coords_special2 <- as.matrix(coords_special2)
########################################################################
library(png)
V(g1)$x <- coords_special2[,1]
V(g1)$y <- coords_special2[,2]
xlim <- c(min(V(g1)$x), max(V(g1)$x))
ylim <- c(min(V(g1)$y), max(V(g1)$y))
# Create the colored clusters
g_by_cluster <- lapply(id_com, function(i) {
this_cluster_ids <- myDataCorrect$X_N[myDataCorrect$X_C == i] %>% as.character()
gtmp <- induced_subgraph(g1, this_cluster_ids)
E(gtmp)$color <- paste(color_palette[i], "1A", sep = "")
if (i == length(id_com)) {
print("last cluster transparent")
E(gtmp)$color <- "#00000000"
}
return(gtmp)
})
# plot with my default params
myplot <- function(network, ...) {
plot(network,
rescale = FALSE,
xlim = xlim,
ylim = ylim,
layout = matrix(c(V(network)$x, V(network)$y), ncol = 2),
vertex.size = 0,
vertex.color = NA,
vertex.frame.color = NA,
vertex.label = NA,
...)
}
# Libraries
library(reticulate)
library(glue)
##########################################################
# Load libraries
source("04_utils/02_libraries.R")
# Load settings from the project we are interested in
source("settings.R")
# One time operation to generate a python env
#reticulate::conda_create(envname = 'openai_env', packages = 'openai', python_version = '3.9')
# Activate enviroment
reticulate::use_condaenv('openai_env')
# Attach key.
# In VSCode create a file `openai.key`
# Is only one line with the OpenAi key.
# `credentials/openai.key` was added to .gitignore so is not committed to the repo.
# import Openai Python library
openai <- reticulate::import('openai')
client = openai$OpenAI(api_key = readr::read_file('05_assets/credentials/openai.key'))
# utils
#' @description
#' Get answers from OpenAI's GPT. Here used for summarization.
#' @param prompt LIST. A prompt in the format of OpenAI. See the code `zz-prompts.R` for details.
#' @param model STRING {gpt-3.5-turbo-0613} the OpenAI Moodel to use. Options: gpt-3.5-turbo-0613, gpt-4, 'gpt-4-0613'
#' @param temperature NUMBER. Between 0 and 2. 0 means less randomness and 2 more creative.
#' @param max_tokens INTEGER. The approx MAX size possible for the reply from ChatGPT.
#' @param n INTEGER. Number of reply variations to get.
#' @returns The JSON reply from OpenAI in R's LIST form. The actual reply text is located at `x$choices[[1]]$message$content`
ask_gpt <- function(prompt,
model = 'gpt-3.5-turbo-0125',
temperature = 0.1,
max_tokens = 500,
n = 1) {
response <- client$chat$completions$create(model = model,
messages = prompt,
temperature = temperature,
max_tokens = as.integer(max_tokens),
n = as.integer(n))
}
#' @description
#' Function to get a subset of the cluster containing the combination of
#' top 5 most linked (X_E), most cited (Z9), and Most linked of the most recent
#' @param dataset DATAFRAME. the dataset
#' @param cluster INTEGER. the cluster number to subset. Compatible with X_C, meaning sypport for cluster 99.
#' @returns DATAFRAME. The largest possible is of `top * 3` when all 3 conditions are different
get_cluster_data <- function(dataset, cluster, top = 5) {
cluster_data <- subset(dataset, X_C == cluster, select = c('X_C','TI','AB','AU','PY','UT','Z9','X_E', 'summary'))
if (nrow(cluster_data) > top) {
selected_papers <- c(
# Most connected
cluster_data$UT[order(cluster_data$X_E, decreasing = TRUE)][1:top],
# Most cited
cluster_data$UT[order(cluster_data$Z9, decreasing = TRUE)][1:top],
# Newest most connected (X_E is preferred over Z9 because most of paper wont have citations)
cluster_data$UT[order(cluster_data$PY, cluster_data$X_E, decreasing = TRUE)][1:top]
) %>% unique()
# Only retain selected papers
cluster_data <- cluster_data[cluster_data$UT %in% selected_papers,]
}
cluster_data$text <- paste(cluster_data$TI, cluster_data$AB, sep = ' ')
return(cluster_data)
}
#' @description
#' AskGPT to summarize each article in the given dataset. Each summary is appended to column `summary`
#' @param dataset DATAFRAME. the dataset
#' @returns DATAFRAME. the same dataset with the column summary appended.
get_papers_summary <- function(cl_dataset) {
#cl_dataset$summary <- ''
starting <- 1
ending <- nrow(cl_dataset)
while(starting < ending) {
for(idx in c(starting:ending)) {
print(paste(cl_dataset$X_C[idx], as.character(idx), cl_dataset$TI[idx], sep = "; "))
article_summary <- tryCatch({
if (nchar(cl_dataset$summary[idx]) == 0) {
article_summary <- ask_gpt(prompt_summarize_a_paper(topic = MAIN_TOPIC,
topic_description = MAIN_TOPIC_DESCRIPTION,
article_text = cl_dataset$text[idx]),
temperature = 0.7)
cl_dataset$summary[idx] <- article_summary$choices[[1]]$message$content
#article_summary
}
},
error = function(err){
message(glue('error found in {idx}'))
message(err)
},
finally = {
starting <- idx
Sys.sleep(5)
})
}
#starting <- idx
}
return(cl_dataset)
}
###################################
###################################
# Initialize
###################################
rcs_merged$description <- ''
rcs_merged$name <- ''
dataset$summary <- ''
source("zz-chatGPT_0_prompts.R")
###################################
###################################
# Article summary
###################################
# The oldest article(s) in the dataset.
# When there are many "old papers" we analyze only the two most cited.
oldest_year <- min(dataset$PY, na.rm = TRUE)
oldest_data <- subset(dataset, PY == oldest_year)
if (nrow(oldest_data) > 2) {
oldest_data <- oldest_data[order(oldest_data$Z9, decreasing = FALSE)[c(1:2)],]
}
oldest_data$summary <- ''
for (i in nrow(oldest_data)) {
old_UT <- oldest_data$UT[i]
old_summary <- ask_gpt(prompt_summarize_a_paper(topic = MAIN_TOPIC,
topic_description = MAIN_TOPIC_DESCRIPTION,
article_text = paste(oldest_data$TI[i], oldest_data$AB[i], sep = ' ')))
oldest_data$summary[i] <- old_summary$choices[[1]]$message$content
dataset$summary[which(dataset$UT == old_UT)] <- old_summary$choices[[1]]$message$content
}
# The following are needed but they are covered in the next block.
# The most cited article in the dataset
# The top 3 most connected per cluster
# The top 3 most cited per cluster
###################################
###################################
# Cluster description and name
###################################
# Only needed once
# rcs_merged$description <- ''
# rcs_merged$name <- ''
# dataset$summary <- ''
#list_of_clusters <- dataset$X_C %>% unique() %>% sort()
# Start where the loop was interrupted
list_of_clusters <- dataset$X_C %>% unique() %>% sort()
list_of_clusters <- list_of_clusters[list_of_clusters != 99]
# list_of_clusters <- list_of_clusters[c(13:length(list_of_clusters))]
# list_of_clusters <- list(5)
for (cluster in list_of_clusters) {
# Get this cluster tops
print('=================================================================')
print(glue('cluster: {cluster}'))
cluster_data <- get_cluster_data(dataset, cluster = cluster, top = 3)
# Summarize each of the selected papers
cluster_data <- get_papers_summary(cluster_data)
# Assign the summaries to the main dataset
print('asign summaries to main dataset')
dataset$summary[match(cluster_data$UT, dataset$UT)] <- cluster_data$summary
# Generate the bulk text
print('get bulk text')
print(glue('Total selected papers for this cluster: {nrow(cluster_data)}'))
my_texts <- list()
for (i in c(1:min(10,nrow(cluster_data)))) {
my_texts[i] <- glue('##### {cluster_data$text[[i]]}')
}
#print(length(my_texts))
my_texts <- paste(my_texts, collapse = ' ')
my_texts <- substr(my_texts, 1, (3500 * 4))
# Get the topic of the cluster
print('Get cluster topic')
cluster_completed <- FALSE
while(!cluster_completed) {
tmp <- tryCatch({
cluster_description <- ask_gpt(prompt_cluster_description(topic = MAIN_TOPIC,
topic_description = MAIN_TOPIC_DESCRIPTION,
cluster_text = my_texts),
model='gpt-3.5-turbo-0125',#'gpt-4',
temperature = 0.2)
cluster_description <- cluster_description$choices[[1]]$message$content
cluster_completed <- TRUE
cluster_description
},
error = function(err){
message(glue('Error getting topic description of cluster {cluster}. Trying again'))
message(err)
})
}
rcs_merged$description[which(rcs_merged$cluster_code == cluster)] <- cluster_description
# Get the name of the cluster
print('Get cluster name')
cluster_completed <- FALSE
while(!cluster_completed) {
tmp <- tryCatch({
cluster_name <- ask_gpt(prompt_cluster_name(topic = MAIN_TOPIC,
topic_description = MAIN_TOPIC_DESCRIPTION,
cluster_description = cluster_description),
model='gpt-3.5-turbo-0125',#'gpt-4',
max_tokens = 60,
temperature = 0.3)
cluster_name <- cluster_name$choices[[1]]$message$content
cluster_completed <- TRUE
cluster_name
},
error = function(err){
message(glue('Error getting topic name of cluster {cluster}. Trying again'))
message(err)
})
}
rcs_merged$name[which(rcs_merged$cluster_code == cluster)] <- cluster_name
}
# We do this to keep copy of the edits in case we mess it.
rcs_merged$name2 <- gsub('^.*?"','',rcs_merged$name) %>% gsub('".$','', .) %>% gsub('"','', .)
rcs_merged$cluster_name <- rcs_merged$name2
rcs_merged$cluster_name[rcs_merged$cluster_code == 99] <- 'Others'
rcs_merged$detailed_description <- rcs_merged$description
for (cluster in list_of_clusters) {
print('=================================================================')
print(glue('cluster: {cluster}'))
# Get the topic of the cluster
print('Get enhanced description')
cluster_completed <- FALSE
while(!cluster_completed) {
tmp <- tryCatch({
cluster_description <- ask_gpt(prompt_cluster_description_enhanced(cluster_description = rcs_merged$detailed_description[rcs_merged$cluster == cluster]),
model='gpt-3.5-turbo-0125',#'gpt-4',
temperature = 1)
cluster_description <- cluster_description$choices[[1]]$message$content
cluster_completed <- TRUE
},
error = function(err){
message(glue('Error getting topic enhanced description of cluster {cluster}. Trying again'))
message(err)
})
}
rcs_merged$description[which(rcs_merged$cluster_code == cluster)] <- cluster_description
}
colnames(rcs_merged)
test <- rcs_merged[,c("cluster","cluster_name", "description")]
write.csv(test, file='Q293_llm_names.csv')
#network <- read.csv("file.ncol", sep = " ", header = FALSE, stringsAsFactors = FALSE)
g1 <- graph_from_data_frame(network, directed = FALSE)
# List of clusters
id_com <- sort(unique(dataset$level0))
# Change last cluster, 99, to natural number
id_com[[length(id_com)]] <- length(id_com)
# Create correspondence table for nodes and clusters
node_cluster <- myDataCorrect[,c("X_N", "level0")]
setnames(node_cluster, c("level0"), c("X_C"))
node_cluster$X_C[node_cluster$X_C == 99] <- length(id_com)
# Sort it from last to first cluster
node_cluster <- node_cluster[order(node_cluster$X_C, decreasing = TRUE),]
View(node_cluster)
# Sort the graph vertices in same order as node_cluster
# There are differences in the count of nodes. After inspecting them I reached these conclusions:
# As long as the dataset has the lowest number seems to be no problem
# Need to check how I built the ncol_file. As I did not filter the network only for the nodes in the dataset. It has other nodes too.
nodes_database <- node_cluster$X_N
nodes_g1 <- names(V(g1)) %>% as.numeric()
g1 <- induced_subgraph(g1, which(nodes_g1 %in% nodes_database))
vv <- V(g1) %>% names %>% as.numeric #Order of nodes in the graph
idx <- match(vv, node_cluster$X_N) #Index to reorder graph
g1 <- permute(g1, idx)
###########################################################################
# Set the GLOBAL layout
# We choose to compute the centroids of clusters based on DRL
# Given that is the one with more distributed effects.
coords_igraph_drl <- layout_with_drl(g1)
# Verify the assignation is correct. It must be TRUE
all(as.numeric(names(V(g1))) == node_cluster$X_N)
########################################################################
# Create color palette
fukan_colors <- c("#f00f15","#2270e7","#e5e510","#ff8103","#4f3dd1","#26cc3a","#ec058e","#9cb8c2","#fffdd0","#b40e68")
fukan_colors_extended <- c(fukan_colors, tolower(c("#5AFB5A", "#BEAED4", "#FDC086", "#99FDFF", "#C430FF", "#E4DBE0", "#BF5B17", "#666666")))#RColorBrewer::brewer.pal(8, "Accent"))
color_palette <- rep_len(fukan_colors_extended, length(id_com))
# Get the pairs of X_N composing each edge
edge_list <- as.data.frame(ends(g1, E(g1)), stringsAsFactors = FALSE)
# From node name to cluster
node_to_cluster <- node_cluster$X_C
names(node_to_cluster) <- as.character(as.integer(node_cluster$X_N))
edge_list[,3] <- node_to_cluster[(edge_list[,1])]
edge_list[,4] <- node_to_cluster[(edge_list[,2])]
# Intracluster links
edge_list[,5] <- edge_list[,3] == edge_list[,4]
table(edge_list[,5])
# From cluster to color
# edge_list[,7] <- adjustcolor(color_palette[edge_list[,6]], alpha.f = 0.4)
edge_list[,7] <- paste(color_palette[edge_list[,6]], "66", sep = "")
# Color the edge. Either strategy will color intracluster edges the same.
# Edge color based on largest-cluster incident node
# edge_list[,6] <- sapply(c(1:nrow(edge_list)), function(x) {max(edge_list[x,3], edge_list[x,4])})
# Edge color based on smallest-cluster incident node <- This is preferred because we can
edge_list[,6] <- sapply(c(1:nrow(edge_list)), function(x) {min(edge_list[x,3], edge_list[x,4], na.rm = TRUE)})
# From cluster to color
# edge_list[,7] <- adjustcolor(color_palette[edge_list[,6]], alpha.f = 0.4)
edge_list[,7] <- paste(color_palette[edge_list[,6]], "66", sep = "")
# Custom modifications
# Last cluster "99" transparent
# Intercluster edges transparent
#edge_list[,7][!edge_list[,5]] <- "#00000000"
edge_list[,7][edge_list[,6] == max(id_com)] <- "#00000000"
max(id_com)
########################################################################
# Special Layout
coords_all_df <- data.frame('node' = V(g1)$name %>% as.numeric(),
'x'= coords_igraph_drl[,1],
'y'= coords_igraph_drl[,2])
coords_all_df <- merge(coords_all_df,
myDataCorrect[,c('X_N', 'X_C')],
by.x = 'node',
by.y = 'X_N',
all.x = TRUE,
all.y = FALSE)
coords_all_centers <- coords_all_df %>%
group_by(X_C) %>%
summarize(mean_x = mean(x, na.rm=TRUE),
mean_y = mean(y, na.rm=TRUE))
coords_all_centers$X_C[coords_all_centers$X_C == 99] <- nrow(coords_all_centers)
coords_all_centers$mean_x <- scale(coords_all_centers$mean_x)
coords_all_centers$mean_y <- scale(coords_all_centers$mean_y)
########################################################################
# Each cluster is plot with LGL
coords_special <- lapply(c(id_com), function(i) {
print(i)
this_cluster_ids <- myDataCorrect$X_N[myDataCorrect$X_C == i] %>% as.character()
gtmp <- induced_subgraph(g1, this_cluster_ids)
gtmp_coords <- layout_with_lgl(gtmp) %>% as.data.frame()
colnames(gtmp_coords) <- c('x','y')
# Treat outliers
bpx <- boxplot(gtmp_coords$x, plot = FALSE)
gtmp_coords$x[gtmp_coords$x > bpx$stats[5,1]] <- bpx$stats[5,1]
gtmp_coords$x[gtmp_coords$x < bpx$stats[1,1]] <- bpx$stats[1,1]
bpy <- boxplot(gtmp_coords$y, plot = FALSE)
gtmp_coords$y[gtmp_coords$y > bpy$stats[5,1]] <- bpy$stats[5,1]
gtmp_coords$y[gtmp_coords$y < bpy$stats[1,1]] <- bpy$stats[1,1]
gtmp_coords_df <- data.frame('node' = V(gtmp)$name,
'x' = scale(gtmp_coords[,1]) +
coords_all_centers$mean_x[coords_all_centers$X_C == i] * 1.5,
'y' = scale(gtmp_coords[,2]) +
coords_all_centers$mean_y[coords_all_centers$X_C == i] * 1.5)
}) %>% rbind.fill()
coords_special2 <- coords_special[match(V(g1)$name, coords_special$node),]
all(coords_special2$node == V(g1)$name)
coords_special2 <- coords_special2[,c('x','y')]
rownames(coords_special2) <- NULL
coords_special2 <- as.matrix(coords_special2)
########################################################################
library(png)
V(g1)$x <- coords_special2[,1]
V(g1)$y <- coords_special2[,2]
xlim <- c(min(V(g1)$x), max(V(g1)$x))
ylim <- c(min(V(g1)$y), max(V(g1)$y))
# Create the colored clusters
g_by_cluster <- lapply(id_com, function(i) {
this_cluster_ids <- myDataCorrect$X_N[myDataCorrect$X_C == i] %>% as.character()
gtmp <- induced_subgraph(g1, this_cluster_ids)
E(gtmp)$color <- paste(color_palette[i], "1A", sep = "")
if (i == length(id_com)) {
print("last cluster transparent")
E(gtmp)$color <- "#00000000"
}
return(gtmp)
})
# plot with my default params
myplot <- function(network, ...) {
plot(network,
rescale = FALSE,
xlim = xlim,
ylim = ylim,
layout = matrix(c(V(network)$x, V(network)$y), ncol = 2),
vertex.size = 0,
vertex.color = NA,
vertex.frame.color = NA,
vertex.label = NA,
...)
}
########################################################################
# Print the colored full network
png(file="network.png", width=1280, height=800)
par(bg = "black")
myplot(g_by_cluster[[length(g_by_cluster)]])
for (i in rev(id_com[1:length(id_com) - 1])) {
print(i)
gtmp <- g_by_cluster[[i]]
myplot(gtmp, add = TRUE)
}
dev.off()
# Call necessary libraries
library(plyr)
library(Opener5)
library(data.table)
library(dplyr)
library(stringr)
file.choose()
###########################################################################################
# OPTIONS
###########################################################################################
## Query_id
## This has de form Qxxx whith the query number from the query control file
dataset_metadata <- list("query_id" = "Q292",
"fukan_url" = "Not apply. Directly from WOS")
# Open a window to select the directory with the files to merge
paths_to_files = list.files(path = choose.dir(), full.names= TRUE, pattern = "*.txt", recursive = TRUE)
# Are we computing our own citation network?
COMPUTE_NETWORK = TRUE
###########################################################################################
## Path to `/inputs`
# Here 'input' refer to the inputs for clustering.
# From the pov of this very code, this is actually the output folder. Where the files generated by this code will be placed.
bibliometrics_folder <- "C:\\Users\\crist\\OneDrive\\Documentos\\03-bibliometrics"
dir.create(file.path(bibliometrics_folder, dataset_metadata$query_id), showWarnings = FALSE)
# Read each file and store them in a vector
# fread sometimes fails when reading the header, what to do?
list_of_all_files <- lapply(paths_to_files, function(a_path){
data1 <- fread(a_path, sep = "\t", stringsAsFactors = FALSE, check.names = FALSE, encoding = "UTF-8", quote = "")
#data1 <- read_from_wos(a_path) # NOTE: DO NOT USE read_from_wos() from package OPNER5, it cut off the lines before finish it, hence it does not read PY and UT for all rows.
#data1 <- read.table(a_path, sep = '\t', fill = TRUE, stringsAsFactors = FALSE, header = FALSE, check.names = FALSE, quote = "", comment.char="", encoding = "UTF-16")
#data1 <- read.csv(a_path, stringsAsFactors = FALSE, check.names = FALSE)
#data1 <- read.delim(a_path, stringsAsFactors = FALSE, check.names = FALSE, encoding = "UTF-16", sep = "\t")
return(data1)})
# Verify than the files have the expected number of rows: 500. Except for a few that were the tails.
plot(unlist(sapply(list_of_all_files, nrow))) #The number of rows in each file, mostly 500.
# Create the merged dataset
dataset <- rbind.fill(list_of_all_files)
dataset <- as.data.frame(dataset)
if (colnames(dataset)[1] == "V1") {
colnames(dataset) <- c("PT", colnames(dataset)[3:length(colnames(dataset))], "END")
dataset["END"] <- NULL
}
# check for possible errors
# Verify correct reading by inspecting the publication year.
# If several non numeric values are present, it means there, there was a problem reading the files.
names(dataset)[1:20]
# A record without PY, EA, or CY can be NA or "" empty string. We normalize anything to NA
dataset$PY[dataset$PY == ""] <- NA
dataset$EA[dataset$EA == ""] <- NA
dataset$CY[dataset$CY == ""] <- NA
# Correct records without PY
table(dataset$PY)
table(is.na(dataset$PY))
dataset$PY[is.na(dataset$PY)] <- 2024
# Correct records without PY
table(dataset$PY)
table(is.na(dataset$PY))
dataset$TI[is.na(dataset$PY)]
# Remove duplicated files
dataset = dataset[!duplicated(dataset$UT),]
#dataset <- dataset[,usable_columns]
dataset$PY <- as.character(dataset$PY)
for (i in c(1:ncol(dataset))) {
dataset[,i]  <- as.character(dataset[,i]) %>% enc2utf8()
}
# Compute network if needed
if (COMPUTE_NETWORK) {
source('./01_data_loading/compute_direct_citation_network.R')
} else {
network <- data.frame()
orphans <- data.frame()
dataset_original <- data.frame()
}
## Create directories
save(dataset,
dataset_original,
orphans,
network,
dataset_metadata,
file = file.path(bibliometrics_folder, dataset_metadata$query_id, "dataset.rdata"))
rm(list = ls())
renv::restore()
renv::snapshot()
