dataset <- as.data.frame(dataset)
if (colnames(dataset)[1] == "V1") {
colnames(dataset) <- c("PT", colnames(dataset)[3:length(colnames(dataset))], "END")
dataset["END"] <- NULL
}
# check for possible errors
# Verify correct reading by inspecting the publication year.
# If several non numeric values are present, it means there, there was a problem reading the files.
names(dataset)[1:20]
# Remove duplicated records
filter_label <- names(settings$filtering)
# Remove duplicates
if (settings$filtering[[filter_label]]$rows_filter$removed_duplicated_UT) {
dataset <- dataset %>% filter(!duplicated(UT))
}
# A record without PY, EA, or CY can be NA or "" empty string. We normalize anything to NA
dataset$PY[dataset$PY == ""] <- NA
dataset$EA[dataset$EA == ""] <- NA
dataset$CY[dataset$CY == ""] <- NA
# A record without PY, EA, or CY can be NA or "" empty string. We normalize anything to NA
dataset$PY[dataset$PY == ""] <- NA
if ("C1" %in% colnames(dataset)) {
dataset$EA[dataset$EA == ""] <- NA
}
if ("CY" %in% colnames(dataset)) {
dataset$CY[dataset$CY == ""] <- NA
}
# Correct records without PY
table(dataset$PY)
table(is.na(dataset$PY))
# A record without PY, EA, or CY can be NA or "" empty string. We normalize anything to NA
dataset$PY[dataset$PY == ""] <- NA
if ("C1" %in% colnames(dataset)) {
dataset$EA[dataset$EA == ""] <- NA
}
if ("CY" %in% colnames(dataset)) {
dataset$CY[dataset$CY == ""] <- NA
}
# Correct records without PY
table(dataset$PY)
table(is.na(dataset$PY))
# Apply most recent year to NAs
dataset$PY[is.na(dataset$PY)] <- settings$filtering[[filter_label]]$rows_filter$most_recent_year %>% as.numeric()
test <- sapply(dataset$PY, function(x) {
return(nchar(as.character(x)))
}) %>% unname()
dataset <- dataset %>% filter(test == 4)
# Correct records without PY
table(dataset$PY)
table(is.na(dataset$PY))
dataset$TI[is.na(dataset$PY)]
dataset$PY <- as.character(dataset$PY)
for (i in c(1:ncol(dataset))) {
dataset[, i] <- as.character(dataset[, i]) %>% enc2utf8()
}
############################################################################
############################################################################
# Appending Columns
############################################################################
############################################################################
# IDs
dataset <- dataset %>%
mutate(
X_N = c(1:n()),
uuid = UUIDgenerate(n = n())
)
# Load utils
source("zz_utils/zz_auxiliary_functions.R")
# Add Countries column
if ("C1" %in% colnames(dataset)) {
dataset$Countries <- getCountries(dataset$C1)
dataset$IsoCountries <- as.character(getIsoCountries(dataset$Countries))
dataset$IsoCountries <- gsub("NA; |; NA$", "", dataset$IsoCountries)
dataset$IsoCountries <- gsub("; NA", "", dataset$IsoCountries)
print("Countries column has been added")
}
# Add institutions column
if ("C1" %in% colnames(dataset)) {
dataset$Institutions <- as.character(getInstitutions(dataset$C1))
print("Institutions column has been added")
}
############################################################################
############################################################################
# Edits
############################################################################
############################################################################
# Clean abstract
dataset$AB <- enc2utf8(dataset$AB)
#dataset$AB <- gsub("ï¿½|<ca><ca><ca>", " ", dataset$AB)
dataset$AB <- iconv(dataset$AB, from = "UTF-8", to = "ASCII", sub = " ")
dataset$AB <- enc2utf8(dataset$AB)
dataset$AB <- remove_copyright_statements(dataset$AB)
dataset$AB <- remove_word_counts_line(dataset$AB)
############################################################################
############################################################################
# Filtering ----- COLUMNS
############################################################################
############################################################################
available_columns <- intersect(colnames(dataset), c(unlist(settings$filtering[[filter_label]]$columns_filter$columns_selected), "Countries", "IsoCountries", "Institutions"))
dataset <- dataset %>%
select(all_of(c("X_N", "uuid", available_columns)))
##########################
# Save the file
# From the pov of this very code, this is actually the output folder. Where the files generated by this code will be placed.
results_folder_path <- file.path(
output_folder_path,
project_folder_name,
filter_label
)
dir.create(file.path(results_folder_path), showWarnings = FALSE)
write.csv(dataset,
file = file.path(results_folder_path, "dataset_raw_cleaned.csv"),
row.names = FALSE
)
# Write the filtering settings in the same location
writeLines(
RJSONIO::toJSON(settings$filtering,
pretty = TRUE,
auto_unbox = TRUE
),
file.path(results_folder_path, "filtering_settings.json")
)
rm(list = ls())
# ==============================================================================
source("_1_entry_dataset.R")
###############################################################################
# Load dataset
dataset <- readr::read_csv(file.path(
settings$metadata$bibliometrics_directory,
settings$metadata$project_folder,
settings$network$from_filtered_dataset,
"dataset_raw_cleaned.csv"
))
View(dataset)
###############################################################################
###############################################################################
###############################################################################
# Compute network if needed
if (settings$network$get_network & settings$network$network_type == "direct_citation") {
source("./01_data_loading/01z_compute_direct_citation_network.R")
} else {
print("User did not request acitation network")
}
# Add a custom identifier to those articles without UT
blank_ut_index <- which(dataset$UT == "")
if (length(blank_ut_index) > 0 ){
custom_id <- paste("wos:ccc", c(1:length(blank_ut_index)), sep = "")
dataset$UT[blank_ut_index] <- custom_id
}
# Transform each article in the dataset to the "record" format:
# Author, Year, Publication name, vol, page, DOI
# AU, PY, J9, VL, BP, DI
c('J9', 'VL', 'BP') %in% colnames(dataset)
# Transform each article in the dataset to the "record" format:
# Author, Year, Publication name, vol, page, DOI
# AU, PY, J9, VL, BP, DI
all(c('J9', 'VL', 'BP') %in% colnames(dataset))
if (all(c('J9', 'VL', 'BP') %in% colnames(dataset))){
first_author <- gsub("; .*$", "", dataset$AU) %>% gsub(",", "", .)
volume <- paste("V", dataset$VL, sep = "")
page <- paste("P", dataset$BP, sep = "")
article_records <- paste(first_author, dataset$PY, dataset$J9, volume, page, sep = ", ")
# Cleansing
article_records <- gsub(", VNA|, P$", ",", article_records)
article_records <- gsub(", ,|,,", ",", article_records)
article_records <- gsub(", ,|,,", ",", article_records) #Yes, we need it twice.
article_records <- gsub(",$|, $", "", article_records)
article_records <- tolower(article_records)
# Valid articles to use:
# If we would like to have a threshold then put it here
# e.g. Those mentioned at least by 2 articles in the dataset
valid_article_records <-  article_records # No thresholds
} else {
print('A column from J9, Vl, or BP is missing.')
print('Because we are likely dealing with Derwent data')
}
# Get and format the dois from the articles in the dataset.
if ('DI' %in% colnames(dataset)) {
article_dois <- dataset$DI %>% unlist %>% tolower
valid_article_dois <-  article_dois[article_dois!=""]
} else {
print('This dataset does not have DI column!')
}
# Transform each article in the dataset to the "record" format:
# Author, Year, Publication name, vol, page, DOI
# AU, PY, J9, VL, BP, DI
rm(valid_article_records)
# Transform each article in the dataset to the "record" format:
# Author, Year, Publication name, vol, page, DOI
# AU, PY, J9, VL, BP, DI
if (all(c('J9', 'VL', 'BP') %in% colnames(dataset))){
first_author <- gsub("; .*$", "", dataset$AU) %>% gsub(",", "", .)
volume <- paste("V", dataset$VL, sep = "")
page <- paste("P", dataset$BP, sep = "")
article_records <- paste(first_author, dataset$PY, dataset$J9, volume, page, sep = ", ")
# Cleansing
article_records <- gsub(", VNA|, P$", ",", article_records)
article_records <- gsub(", ,|,,", ",", article_records)
article_records <- gsub(", ,|,,", ",", article_records) #Yes, we need it twice.
article_records <- gsub(",$|, $", "", article_records)
article_records <- tolower(article_records)
# Valid articles to use:
# If we would like to have a threshold then put it here
# e.g. Those mentioned at least by 2 articles in the dataset
valid_article_records <-  article_records # No thresholds
} else {
print('A column from J9, Vl, or BP is missing.')
print('Because we are likely dealing with Derwent data')
}
# Get and format the dois from the articles in the dataset.
if ('DI' %in% colnames(dataset)) {
article_dois <- dataset$DI %>% unlist %>% tolower
valid_article_dois <-  article_dois[article_dois!=""]
} else {
print('This dataset does not have DI column!')
}
# Get the cited references
# For each article in the dataset, we get a list of the cited references from the CR column.
references <- strsplit(dataset$CR, "; ")
# Count the number of references per article
number_of_references <- sapply(references, length)
# Unlisted references. This unnest the reference object.
references_unlisted <- unlist(references) %>% tolower()
rm(references)
# Create a vector of article indexes (To be appended to the cited reference list)
# The index of a given article is repeated n times, where n is the number of references found in that article
# If the article does not have references, then a placeholder "numeric(0)" is returned.
# The placeholders are dropped once unlisted, so they don't affect the computation.
indices <- sapply(c(1:length(article_records)), function(x){
rep(x, number_of_references[x])
}) %>% unlist
# Create the association table.
# This is the table to be used to match the cited references and the articles in our dataset.
# First, we verify that indices and the references are of same length.
# Then, we transform both as a single data.frame
# Hence, the "article_index_ column tells us the article to which the "reference_records" belongs to.
if (length(indices) == length(references_unlisted)) {
references_df <- data.frame("article_index" = indices,
"reference_records" = references_unlisted,
stringsAsFactors = FALSE)
}
View(references_df)
rm(references_unlisted)
View(references_df)
View(references_df)
# DOIs are the last element of the reference. Not all articles have the DOI.
# Here we separate the DOI and put it as a different column.
# It will help to match the cited references to the articles in our dataset by comparing
# either the formated citation or the DOI.
if (all(c('J9', 'VL', 'BP') %in% colnames(dataset))){
separated <- str_split_fixed(references_df$reference_records, ", doi ", 2)
references_df$reference_records <- separated[, 1]
references_df$reference_dois <- separated[, 2]
rm(separated)
} else {
references_df$reference_dois <- references_df$reference_records
references_df$reference_records <- ''
}
View(references_df)
format(object.size(references_df), units = "Gb")
# Check object size
format(object.size(references_df), units = "Gb")
# Let only valid references
# The list of cited references also contains references to paper outside our dataset (or even outside of WOS)
# The citation network approach we use in our lab, stablish connection to among the papers within our dataset only.
# Therefore, we must remove any other unncesery reference. As naturally it won't create a match.
if (exists('valid_article_records')) {
references_df$is_valid_reference_record <- references_df$reference_records %in% valid_article_records
} else {
references_df$is_valid_reference_record <- FALSE
}
table(references_df$is_valid_reference_record)
if (exists('valid_article_dois')) {
references_df$is_valid_reference_doi <- references_df$reference_dois %in% valid_article_dois
} else {
references_df$is_valid_reference_doi <- FALSE
}
table(references_df$is_valid_reference_doi)
references_df <- references_df[references_df$is_valid_reference_record|references_df$is_valid_reference_doi,]
###########
# Matching: Create the association between the papers in the dataset and their cited references.
# The objects to compare to are "article_records" and "article_dois" which must have the same size.
###########
# Find those references where only DOI is available
references_df$only_doi <- (!references_df$is_valid_reference_record) & references_df$is_valid_reference_doi
# Create a link from the reference to the article by using the "record" format
references_df$links_to_article_index[references_df$is_valid_reference_record] <- match(references_df$reference_records[references_df$is_valid_reference_record], article_records)
# Create a link from the reference to the article by using the DOI, for those articles where just DOI is available
references_df$links_to_article_index[references_df$only_doi] <- match(references_df$reference_dois[references_df$only_doi], article_dois)
# Compute the network file
ncol_file <- cbind(as.character(references_df$article_index),
as.character(references_df$links_to_article_index)) %>% as.matrix
# As data frame
network <- ncol_file %>% as.data.frame()
###############################################################################
###############################################################################
###############################################################################
# Save
results_folder_path <- file.path(
output_folder_path,
project_folder_name,
settings$network$from_filtered_dataset,
settings$network$network_type
)
dir.create(file.path(results_folder_path), showWarnings = FALSE)
write.csv(network,
file = file.path(results_folder_path, "network.csv"),
row.names = FALSE
)
# Write the filtering settings in the same location
writeLines(
RJSONIO::toJSON(settings$network,
pretty = TRUE,
auto_unbox = TRUE
),
file.path(results_folder_path, "network_settings.json")
)
rm(list = ls())
library(ggrepel)
library(stats)
library(tidyr)
library(reshape2)
heatmap_analysis_id = 'H007_innovation_innovativeness'
settings_directive = 'heatmap_settings_H007_Innovation-Innovativeness.json'
###############################################################################
# Call necessary libraries
source("zz_utils/02_libraries.R")
source("zz_utils/00_system_paths.R")
###############################################################################
# Load the directive file
settings <- RJSONIO::fromJSON(
file.path(
output_folder_path,
heatmap_analysis_id,
settings_directive
),
simplify = FALSE
)
# Save setting inputs as a df.
inputs <- lapply(settings$inputs, function(x) {data.frame(x)}) %>% rbind.fill()
# Save setting inputs as a df.
inputs <- lapply(settings$inputs, function(x) {data.frame(x)}) %>% rbind.fill()
###############################################################################
# Read the files
# The coordinates of all participating clusters across analysis in this heatmap
# Computed in the Heatmap colab
coords <- readr::read_csv(file.path(
output_folder_path,
heatmap_analysis_id,
"coordinates.csv"
)) %>%
select(x, y, cluster) %>%
rename(cluster_code = cluster)
# Read the RCS files
# For the heatmap the RCS is expected to have already
# The documents, PY_Mean, Z9_Mean, and the LLM_name (cluster_name)
rcs <- lapply(c(1:nrow(inputs)), \(x) {
this_df <- readr::read_csv(file.path(
output_folder_path,#settings$metadata$input_directory,
settings$inputs[[x]]$project_folder_name,
settings$inputs[[x]]$analysis_folder_name,
#"louvain",
#"0.9",
settings$inputs[[x]]$level_folder_name,
"cluster_summary_short_dc.csv"#"rcs_merged.csv"
)) %>% mutate(
# Add the dataset name to the RCS cluster code
cluster_code = paste(inputs$display_name[[x]], cluster_code, sep = '-')
)
}) %>% rbind.fill() %>%
select(cluster_code, cluster_name, documents, PY_Mean, Z9_Mean) #Count, ave_PY, ave_Z9)
# Read the RCS files
# For the heatmap the RCS is expected to have already
# The documents, PY_Mean, Z9_Mean, and the LLM_name (cluster_name)
rcs <- lapply(c(1:nrow(inputs)), \(x) {
this_df <- readr::read_csv(file.path(
output_folder_path,#settings$metadata$input_directory,
settings$inputs[[x]]$project_folder_name,
settings$inputs[[x]]$analysis_folder_name,
#"louvain",
#"0.9",
settings$inputs[[x]]$level_folder_name,
"cluster_summary.csv"#"rcs_merged.csv"
)) %>% mutate(
# Add the dataset name to the RCS cluster code
cluster_code = paste(inputs$display_name[[x]], cluster_code, sep = '-')
)
}) %>% rbind.fill() %>%
select(cluster_code, cluster_name, documents, PY_Mean, Z9_Mean) #Count, ave_PY, ave_Z9)
###############################################################################
###############################################################################
# Merge the datasets and create needed columns
tmp <- merge(rcs, coords, by = 'cluster_code')
tmp <- tmp %>%
separate(cluster_code,
remove = FALSE,
into = c("dataset", "local_cluster"),
sep = "-",
extra = "merge")
# When using subclusters, lets remove the trailing "---"
tmp$local_cluster <- gsub("---", "", tmp$local_cluster)
tmp <- merge(tmp,
inputs %>%
select(display_name, color, heatmap_display_order, sankey_display_order) %>%
rename(dataset = display_name),
by = 'dataset')
# The labels shown in the scatter plots
tmp$scatter_labels <- paste(tmp$local_cluster, tmp$cluster_name, sep = ':')
# Groups of clusters
km1 <- kmeans(tmp[,c("x","y")], centers = floor(sqrt(nrow(rcs))))
tmp$group <- as.factor(km1$cluster)
View(tmp)
table(tmp$dataset)
View(tmp)
tmp$dataset <- gsub('INVN', 'IN', tmp$dataset)
tmp$dataset <- gsub('INVTNS', 'IS', tmp$dataset)
tmp$cluster_code <- gsub('INVN', 'IN', tmp$cluster_code)
tmp$cluster_code <- gsub('INVTNS', 'IS', tmp$cluster_code)
View(tmp)
###############################################################################
###############################################################################
# Get the scatter plots
# Aux plot function
plot_scatter_group <- function(rcs_data,
point_labels,
x_column,
y_column,
color_hex_column,
color_labels,
size_column,
x_column_label = x_column,
y_column_label = y_column,
show_tags = TRUE) {
# format the df
df <- rcs_data[, c(point_labels, x_column, y_column, color_hex_column, color_labels, size_column)]
colnames(df) <- c("point_labels", "x",    "y",       "color_hex",      "color_label", "size")
# plot
p <- ggplot(df, aes(x = x, y = y)) +
stat_ellipse(geom = "polygon",
aes(linetype = rcs_data$group),
alpha=0.07) + #0.07 OR 0.35 for colors
geom_point(aes(color = color_hex,
size = size)) +
scale_color_identity() +
xlab("") +
ylab("")
if (show_tags) {
p <- p + geom_text_repel(aes(label = gsub("---", "", point_labels)), max.overlaps = 30, size = 2)
}
p <- p + theme_bw() + theme(legend.position = "none")
p
}
#################################################
# Aux plot function
plot_scatter <- function(rcs_data,
point_labels,
x_column,
y_column,
color_hex_column,
color_labels,
size_column,
min_x,
max_x,
max_y,
x_column_label = x_column,
y_column_label = y_column,
show_tags = TRUE) {
# format the df
df <- rcs_data[, c(point_labels, x_column, y_column, color_hex_column, color_labels, size_column)]
colnames(df) <- c("point_labels", "x",    "y",       "color_hex",      "color_label", "size")
# plot
p <- ggplot(df, aes(x = x, y = y)) +
geom_point(aes(colour = color_hex,
size = size)) +
scale_color_identity() +
scale_x_continuous(limits = c(floor(min_x), ceiling(max_x))) + # = seq(2005, 2022, by = 2)) +
#scale_x_continuous(breaks = seq(2005, 2022, by = 2)) +
scale_y_continuous(limits = c(0, (round(max_y, 0) + 10))) +
xlab("") +
ylab("")
if (show_tags) {
p <- p + geom_text_repel(aes(label = gsub("---", "", point_labels)), max.overlaps = 15, size = 5)
}
p <- p + theme_bw() + theme(legend.position = "none")
p
}
#################################################
# Aux function to map values to a desired range
# Here we use it to remap the "Value" links of the Sankeys.
map_to_range <- function(x, new_min, new_max) {
# Handle edge case where all values are the same
if (max(x, na.rm = TRUE) == min(x, na.rm = TRUE)) {
return(rep(new_min, length(x)))
}
# First normalize to 0-1 range, then scale to new range
x_std <- (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
round(new_min + (new_max - new_min) * x_std, 0)
}
#################################################
# This prevents R generating unnecessary error from ggrepel
options(warn = 1)
# Global scatter by x and y MDS scalling
# i.e. Topic Model plot
tm_plot <- plot_scatter_group(tmp,
point_labels = "cluster_code",
x_column = "x",
y_column = "y",
color_hex_column = "color",
color_labels = "dataset",
size_column = "documents",
show_tags = TRUE)
tm_plot
