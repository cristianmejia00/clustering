settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
"dataset.rdata"
))
# Auxiliary code to find the right number of clusters. And update the threshold.
# Get the clusters collecting 90% of papers or the top 10, whatever is the smallest number.
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum %>% plot
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum
table(dataset$X_C) %>% sort(decreasing = TRUE) %>%  plot()
##########################################################
# Check all documents have a cluster assigned
if (any(is.na(dataset$X_C))) {
print('CRITICAL: At least one document is missing cluster assignation!')
print('Those papers are removed')
dataset <- dataset[!is.na(dataset$X_C),]
}
##########################################################
# Document classification (Get clusters or Get topics)
if (settings$params$type_of_analysis == "citation_network") {
source(file.path(getwd(), "02_citation_network", "00_citation_network_clustering.R"))
}
if (settings$params$type_of_analysis == "topic_model") {
source(file.path(getwd(), "02_topic_model", "00_topic_model_clustering.R"))
}
# Create stats folder
dir.create(file.path(
settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder
))
source(file.path(getwd(), "03_reports", "03_general_summary.R"))
# Orphans treatment
if (settings$addons$include_orphans == "99" | settings$addons$include_orphans == "999") {
source(file.path(getwd(), "04_utils", "zz-append_orphans.R"))
}
# Add-ons
if (settings$params$type_of_analysis == "citation_network" &
exists('g1') &
(settings$addons$page_rank | settings$addons$eigen_centrality | settings$addons$closeness_centrality | settings$addons$betweeness_centrality)) {
source(file.path(getwd(), "04_utils", "zz-centrality_meassures.R"))
}
##########################################################
# save objects
if (settings$params$type_of_analysis == "topic_model") {
dataset <- myDataCorrect
}
# Auxiliary code to find the right number of clusters. And update the threshold.
# Get the clusters collecting 90% of papers or the top 10, whatever is the smallest number.
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum %>% plot
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum
table(dataset$X_C) %>% sort(decreasing = TRUE) %>%  plot()
##########################################################
# Select root directory
# It should be the directory where this code is placed.
# setwd("/var/container/MAIN TOPIC-CLUSTERING") #Linux
# setwd(choose.dir()) #Windows
getwd()
##########################################################
# Load libraries
source("04_utils/02_libraries.R")
# Load input settings file
source("settings.R")
# Load data
load(file.path(
settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
"dataset.rdata"
))
# Auxiliary code to find the right number of clusters. And update the threshold.
# Get the clusters collecting 90% of papers or the top 10, whatever is the smallest number.
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum %>% plot
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum
table(dataset$X_C) %>% sort(decreasing = TRUE) %>%  plot()
table(dataset$X_C)
##########################################################
# Check all documents have a cluster assigned
if (any(is.na(dataset$X_C))) {
print('CRITICAL: At least one document is missing cluster assignation!')
print('Those papers are removed')
dataset <- dataset[!is.na(dataset$X_C),]
}
##########################################################
# Document classification (Get clusters or Get topics)
if (settings$params$type_of_analysis == "citation_network") {
source(file.path(getwd(), "02_citation_network", "00_citation_network_clustering.R"))
}
if (settings$params$type_of_analysis == "topic_model") {
source(file.path(getwd(), "02_topic_model", "00_topic_model_clustering.R"))
}
# Auxiliary code to find the right number of clusters. And update the threshold.
# Get the clusters collecting 90% of papers or the top 10, whatever is the smallest number.
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum %>% plot
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum
table(dataset$X_C) %>% sort(decreasing = TRUE) %>%  plot()
table(dataset$X_C)
# Create stats folder
dir.create(file.path(
settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder
))
source(file.path(getwd(), "03_reports", "03_general_summary.R"))
# Orphans treatment
if (settings$addons$include_orphans == "99" | settings$addons$include_orphans == "999") {
source(file.path(getwd(), "04_utils", "zz-append_orphans.R"))
}
# Add-ons
if (settings$params$type_of_analysis == "citation_network" &
exists('g1') &
(settings$addons$page_rank | settings$addons$eigen_centrality | settings$addons$closeness_centrality | settings$addons$betweeness_centrality)) {
source(file.path(getwd(), "04_utils", "zz-centrality_meassures.R"))
}
##########################################################
# save objects
if (settings$params$type_of_analysis == "topic_model") {
dataset <- myDataCorrect
}
save(dataset, file = file.path(settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder,
"dataset_clustering.rdata"))
save(network, file = file.path(settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder,
"network.rdata"))
save.image(file.path(settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder,
"environ_clustering.rdata"))
rm(list = ls())
##########################################################
# Load libraries
source("04_utils/02_libraries.R")
# Load settings from the project we are interested in
# source(file.choose())
source("settings.R")
##########################################################
# Output Folder
output_folder_reports <- file.path(settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder)
dir.create(output_folder_reports)
##########################################################
# Load data
load(file.path(
settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder,
"dataset_clustering.rdata"
))
##########################################################
# Verify the data is correctly formatted for reports
source(file.path(getwd(), "04_utils", "00_verify_data.R"))
#dataset$X_E <- dataset$Z9
dataset$X_E[is.na(dataset$X_E)] <- 0
zz_env <- list('x01' = ls())
# Reporting clusters
source(file.path(getwd(), "02_citation_network", "01_execute_and_reports.R"))
# Save code snapshot
files_to_save <- list.files(getwd(), full.names = TRUE, recursive = TRUE)
files_to_omit <- list.files(file.path(getwd(),'renv','library'), full.names = TRUE, recursive = TRUE)
files_to_save <- setdiff(files_to_save, files_to_omit)
# Not to zip Rdata environments as they are heavy and saved separately
files_to_save <- files_to_save[!grepl('rdata$', tolower(files_to_save))]
# Zip them. This needs Rtools to work
zip(zipfile = file.path(output_folder_level, 'source_code'),
files = files_to_save)
# Save readable settings
writeLines(RJSONIO::toJSON(settings, pretty=TRUE, auto_unbox=TRUE),
file.path(output_folder_level, "settings.json"))
# Save settings object
save(settings, file = file.path(output_folder_level, "settings.rdata"))
# Save package list
session_info <- sessionInfo()
save(session_info, file = file.path(output_folder_level, "sessionInfo.rdata"))
writeLines(capture.output(sessionInfo()), file.path(output_folder_level, "sessionInfo.txt"))
# Save Global environment
save.image(file.path(output_folder_level, "environ_zz_reports.rdata"))
# Save cluster IDS
if ('fukan_original_cluster_id' %in% colnames(dataset)) {
print('Saving cluster id comparison for subclusters')
cluster_comparison <- dataset[c('X_C', 'fukan_X_C', 'fukan_original_cluster_id', 'fukan_subcluster_label')]
cluster_comparison <- cluster_comparison[!duplicated(cluster_comparison$fukan_subcluster_label),]
cluster_comparison <- cluster_comparison[order(cluster_comparison$fukan_X_C),]
write.csv(cluster_comparison, file = file.path(output_folder_level, "cluster_id_comparison.csv"), row.names = FALSE)
}
# Call necessary libraries
library(plyr)
library(Opener5)
library(data.table)
library(dplyr)
library(stringr)
choose.files()
file.choose()
###########################################################################################
# OPTIONS
###########################################################################################
## Query_id
## This has de form Qxxx whith the query number from the query control file
dataset_metadata <- list("query_id" = "Q295",
"fukan_url" = "Not apply. Directly from WOS")
###########################################################################################
# OPTIONS
###########################################################################################
## Query_id
## This has de form Qxxx whith the query number from the query control file
dataset_metadata <- list("query_id" = "Q296",
"fukan_url" = "Not apply. Directly from WOS")
# Open a window to select the directory with the files to merge
paths_to_files = list.files(path = choose.dir(), full.names= TRUE, pattern = "*.txt", recursive = TRUE)
# Are we computing our own citation network?
COMPUTE_NETWORK = TRUE
###########################################################################################
## Path to `/inputs`
# Here 'input' refer to the inputs for clustering.
# From the pov of this very code, this is actually the output folder. Where the files generated by this code will be placed.
bibliometrics_folder <- "C:\\Users\\crist\\OneDrive\\Documentos\\03-bibliometrics"
dir.create(file.path(bibliometrics_folder, dataset_metadata$query_id), showWarnings = FALSE)
# Read each file and store them in a vector
# fread sometimes fails when reading the header, what to do?
list_of_all_files <- lapply(paths_to_files, function(a_path){
data1 <- fread(a_path, sep = "\t", stringsAsFactors = FALSE, check.names = FALSE, encoding = "UTF-8", quote = "")
#data1 <- read_from_wos(a_path) # NOTE: DO NOT USE read_from_wos() from package OPNER5, it cut off the lines before finish it, hence it does not read PY and UT for all rows.
#data1 <- read.table(a_path, sep = '\t', fill = TRUE, stringsAsFactors = FALSE, header = FALSE, check.names = FALSE, quote = "", comment.char="", encoding = "UTF-16")
#data1 <- read.csv(a_path, stringsAsFactors = FALSE, check.names = FALSE)
#data1 <- read.delim(a_path, stringsAsFactors = FALSE, check.names = FALSE, encoding = "UTF-16", sep = "\t")
return(data1)})
# Verify than the files have the expected number of rows: 500. Except for a few that were the tails.
plot(unlist(sapply(list_of_all_files, nrow))) #The number of rows in each file, mostly 500.
# Create the merged dataset
dataset <- rbind.fill(list_of_all_files)
dataset <- as.data.frame(dataset)
if (colnames(dataset)[1] == "V1") {
colnames(dataset) <- c("PT", colnames(dataset)[3:length(colnames(dataset))], "END")
dataset["END"] <- NULL
}
# check for possible errors
# Verify correct reading by inspecting the publication year.
# If several non numeric values are present, it means there, there was a problem reading the files.
names(dataset)[1:20]
# A record without PY, EA, or CY can be NA or "" empty string. We normalize anything to NA
dataset$PY[dataset$PY == ""] <- NA
dataset$EA[dataset$EA == ""] <- NA
dataset$CY[dataset$CY == ""] <- NA
# Correct records without PY
table(dataset$PY)
table(is.na(dataset$PY))
dataset$PY[is.na(dataset$PY)] <- 2024
# Correct records without PY
table(dataset$PY)
table(is.na(dataset$PY))
dataset$TI[is.na(dataset$PY)]
# Remove duplicated files
dataset = dataset[!duplicated(dataset$UT),]
#dataset <- dataset[,usable_columns]
dataset$PY <- as.character(dataset$PY)
for (i in c(1:ncol(dataset))) {
dataset[,i]  <- as.character(dataset[,i]) %>% enc2utf8()
}
# Compute network if needed
if (COMPUTE_NETWORK) {
source('./01_data_loading/compute_direct_citation_network.R')
} else {
network <- data.frame()
orphans <- data.frame()
dataset_original <- data.frame()
}
## Create directories
save(dataset,
dataset_original,
orphans,
network,
dataset_metadata,
file = file.path(bibliometrics_folder, dataset_metadata$query_id, "dataset.rdata"))
rm(list = ls())
settings <- list()
##########################################################
# Select root directory
# It should be the directory where this code is placed.
# setwd("/var/container/MAIN TOPIC-CLUSTERING") #Linux
# setwd(choose.dir()) #Windows
getwd()
##########################################################
# Load libraries
source("04_utils/02_libraries.R")
# Load input settings file
source("settings.R")
# Load data
load(file.path(
settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
"dataset.rdata"
))
# Auxiliary code to find the right number of clusters. And update the threshold.
# Get the clusters collecting 90% of papers or the top 10, whatever is the smallest number.
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum %>% plot
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum
table(dataset$X_C) %>% sort(decreasing = TRUE) %>%  plot()
table(dataset$X_C)
##########################################################
# Check all documents have a cluster assigned
if (any(is.na(dataset$X_C))) {
print('CRITICAL: At least one document is missing cluster assignation!')
print('Those papers are removed')
dataset <- dataset[!is.na(dataset$X_C),]
}
##########################################################
# Document classification (Get clusters or Get topics)
if (settings$params$type_of_analysis == "citation_network") {
source(file.path(getwd(), "02_citation_network", "00_citation_network_clustering.R"))
}
# Auxiliary code to find the right number of clusters. And update the threshold.
# Get the clusters collecting 90% of papers or the top 10, whatever is the smallest number.
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum %>% plot
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum
table(dataset$X_C) %>% sort(decreasing = TRUE) %>%  plot()
settings <- list()
## Metadata
settings$analysis_metadata <- list(
# Directory path
bibliometrics_folder = "C:\\Users\\crist\\OneDrive\\Documentos\\03-bibliometrics",
project_folder = "Q296",
analysis_folder = "001", # Equivalent to Fukan's analysis (i.e. the order inside dataset)
# Query and data
query = 'Q296',
query_id = "Q296", # This is the Folder name. Equivalent to Fukan's dataset
fukan_url = "Not apply. Compute directly",
downloaded_documents = "969",
# project
project_name = "just transition",
project_description = "Citation network of just transition",
date = "2024-05-01",
created_by = "cristianmejia00@gmail.com",
notes = ""
)
## General Parameters
settings$params <- list(
type_of_dataset = "papers", # "papers", "patents" or "news"
unit_of_analysis = "cluster", # topic, cluster, facet, firm, country, institution, author, etc.
type_of_analysis = "citation_network", # "topic_model" or "citation_network"
dataset_source = "wos", # wos, derwent, factiva (dimensions = wos)
recursive_level = 1,   # Reports will be generated to this level. Topic Models are always 0.
seed = 100 # The seed for random initialization. Needed for reproducibility
)
##########################################################
# Select root directory
# It should be the directory where this code is placed.
# setwd("/var/container/MAIN TOPIC-CLUSTERING") #Linux
# setwd(choose.dir()) #Windows
getwd()
##########################################################
# Load libraries
source("04_utils/02_libraries.R")
# Load input settings file
source("settings.R")
# Load data
load(file.path(
settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
"dataset.rdata"
))
# Auxiliary code to find the right number of clusters. And update the threshold.
# Get the clusters collecting 90% of papers or the top 10, whatever is the smallest number.
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum %>% plot
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum
table(dataset$X_C) %>% sort(decreasing = TRUE) %>%  plot()
table(dataset$X_C)
##########################################################
# Check all documents have a cluster assigned
if (any(is.na(dataset$X_C))) {
print('CRITICAL: At least one document is missing cluster assignation!')
print('Those papers are removed')
dataset <- dataset[!is.na(dataset$X_C),]
}
##########################################################
# Document classification (Get clusters or Get topics)
if (settings$params$type_of_analysis == "citation_network") {
source(file.path(getwd(), "02_citation_network", "00_citation_network_clustering.R"))
}
# Auxiliary code to find the right number of clusters. And update the threshold.
# Get the clusters collecting 90% of papers or the top 10, whatever is the smallest number.
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum %>% plot
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum
table(dataset$X_C) %>% sort(decreasing = TRUE) %>%  plot()
table(dataset$X_C)
# Create stats folder
dir.create(file.path(
settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder
))
source(file.path(getwd(), "03_reports", "03_general_summary.R"))
# Orphans treatment
if (settings$addons$include_orphans == "99" | settings$addons$include_orphans == "999") {
source(file.path(getwd(), "04_utils", "zz-append_orphans.R"))
}
# Add-ons
if (settings$params$type_of_analysis == "citation_network" &
exists('g1') &
(settings$addons$page_rank | settings$addons$eigen_centrality | settings$addons$closeness_centrality | settings$addons$betweeness_centrality)) {
source(file.path(getwd(), "04_utils", "zz-centrality_meassures.R"))
}
##########################################################
# save objects
if (settings$params$type_of_analysis == "topic_model") {
dataset <- myDataCorrect
}
save(dataset, file = file.path(settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder,
"dataset_clustering.rdata"))
save(network, file = file.path(settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder,
"network.rdata"))
save.image(file.path(settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder,
"environ_clustering.rdata"))
vcount(g1)
rm(list = ls())
##########################################################
# Select root directory
# It should be the directory where this code (00_general_parameters.R) is placed.
# setwd("/var/container/MAIN TOPIC-CLUSTERING") #Linux
# setwd(choose.dir()) #Windows
getwd()
##########################################################
# Load libraries
source("04_utils/02_libraries.R")
# Load settings from the project we are interested in
# source(file.choose())
source("settings.R")
##########################################################
# Output Folder
output_folder_reports <- file.path(settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder)
dir.create(output_folder_reports)
##########################################################
# Load data
load(file.path(
settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder,
"dataset_clustering.rdata"
))
##########################################################
# Verify the data is correctly formatted for reports
source(file.path(getwd(), "04_utils", "00_verify_data.R"))
#dataset$X_E <- dataset$Z9
dataset$X_E[is.na(dataset$X_E)] <- 0
zz_env <- list('x01' = ls())
# Reporting clusters
source(file.path(getwd(), "02_citation_network", "01_execute_and_reports.R"))
# Save code snapshot
files_to_save <- list.files(getwd(), full.names = TRUE, recursive = TRUE)
files_to_omit <- list.files(file.path(getwd(),'renv','library'), full.names = TRUE, recursive = TRUE)
files_to_save <- setdiff(files_to_save, files_to_omit)
# Not to zip Rdata environments as they are heavy and saved separately
files_to_save <- files_to_save[!grepl('rdata$', tolower(files_to_save))]
# Zip them. This needs Rtools to work
zip(zipfile = file.path(output_folder_level, 'source_code'),
files = files_to_save)
# Save readable settings
writeLines(RJSONIO::toJSON(settings, pretty=TRUE, auto_unbox=TRUE),
file.path(output_folder_level, "settings.json"))
# Save settings object
save(settings, file = file.path(output_folder_level, "settings.rdata"))
# Save package list
session_info <- sessionInfo()
save(session_info, file = file.path(output_folder_level, "sessionInfo.rdata"))
writeLines(capture.output(sessionInfo()), file.path(output_folder_level, "sessionInfo.txt"))
# Save Global environment
save.image(file.path(output_folder_level, "environ_zz_reports.rdata"))
# Save cluster IDS
if ('fukan_original_cluster_id' %in% colnames(dataset)) {
print('Saving cluster id comparison for subclusters')
cluster_comparison <- dataset[c('X_C', 'fukan_X_C', 'fukan_original_cluster_id', 'fukan_subcluster_label')]
cluster_comparison <- cluster_comparison[!duplicated(cluster_comparison$fukan_subcluster_label),]
cluster_comparison <- cluster_comparison[order(cluster_comparison$fukan_X_C),]
write.csv(cluster_comparison, file = file.path(output_folder_level, "cluster_id_comparison.csv"), row.names = FALSE)
}
# Save code snapshot
files_to_save <- list.files(getwd(), full.names = TRUE, recursive = TRUE)
files_to_omit <- list.files(file.path(getwd(),'renv','library'), full.names = TRUE, recursive = TRUE)
files_to_save <- setdiff(files_to_save, files_to_omit)
# Not to zip Rdata environments as they are heavy and saved separately
files_to_save <- files_to_save[!grepl('rdata$', tolower(files_to_save))]
# Zip them. This needs Rtools to work
zip(zipfile = file.path(output_folder_level, 'source_code'),
files = files_to_save)
# Save readable settings
writeLines(RJSONIO::toJSON(settings, pretty=TRUE, auto_unbox=TRUE),
file.path(output_folder_level, "settings.json"))
# Save settings object
save(settings, file = file.path(output_folder_level, "settings.rdata"))
# Save package list
session_info <- sessionInfo()
save(session_info, file = file.path(output_folder_level, "sessionInfo.rdata"))
writeLines(capture.output(sessionInfo()), file.path(output_folder_level, "sessionInfo.txt"))
# Save Global environment
save.image(file.path(output_folder_level, "environ_zz_reports.rdata"))
# Save cluster IDS
if ('fukan_original_cluster_id' %in% colnames(dataset)) {
print('Saving cluster id comparison for subclusters')
cluster_comparison <- dataset[c('X_C', 'fukan_X_C', 'fukan_original_cluster_id', 'fukan_subcluster_label')]
cluster_comparison <- cluster_comparison[!duplicated(cluster_comparison$fukan_subcluster_label),]
cluster_comparison <- cluster_comparison[order(cluster_comparison$fukan_X_C),]
write.csv(cluster_comparison, file = file.path(output_folder_level, "cluster_id_comparison.csv"), row.names = FALSE)
}
View(stats_size)
