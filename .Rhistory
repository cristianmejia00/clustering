oldest_data$summary[i] <- old_summary
dataset$summary[which(dataset$UT == old_UT)] <- old_summary
}
# Dependency injection. Either `ask_claude`, or `ask_llm`
ask_llm <- ask_claude
###################################
###################################
# Article summary
###################################
# The oldest article(s) in the dataset.
# When there are many "old papers" we analyze only the two most cited.
oldest_year <- min(dataset$PY, na.rm = TRUE)
oldest_data <- subset(dataset, PY <= oldest_year) # subset(dataset, PY == oldest_year)
if (nrow(oldest_data) > 2) {
oldest_data <- oldest_data[order(oldest_data$Z9, decreasing = FALSE)[c(1:2)], ]
}
oldest_data$summary <- ""
for (i in c(1:nrow(oldest_data))) {
print(i)
old_UT <- oldest_data$UT[i]
prompt_old <- prompt_summarize_a_paper(
topic = MAIN_TOPIC,
topic_description = MAIN_TOPIC_DESCRIPTION,
article_text = paste(oldest_data$TI[i], oldest_data$AB[i], sep = " ")
)
old_summary <- ask_llm(
system_prompt = prompt_old$system,
user_prompt = prompt_old$user
)
print(old_summary)
oldest_data$summary[i] <- old_summary
dataset$summary[which(dataset$UT == old_UT)] <- old_summary
}
# The following are needed but they are covered in the next block.
# The most cited article in the dataset
# The top 3 most connected per cluster
# The top 3 most cited per cluster
###################################
###################################
# Cluster description and name
###################################
for (cluster_code in list_of_cluster_codes) {
# Get this cluster tops
print("=================================================================")
print(glue("cluster: {cluster_code}"))
cluster_data <- get_cluster_data(dataset, cluster_ = cluster_code, top = this_tops)
print(cluster_data$X_C)
if (COMPUTE_SUMMARIES) {
print("Computing summaries of selected articles in this cluster...")
# Summarize each of the selected papers
cluster_data <- get_papers_summary(cluster_data)
# Assign the summaries to the main dataset
print("asign summaries to main dataset")
dataset$summary[match(cluster_data$UT, dataset$UT)] <- cluster_data$summary
} else {
print("Article summaries were not requested for this cluster.")
cluster_data$text <- paste(cluster_data$TI, cluster_data$AB, sep = " ")
}
# Generate the bulk text
print("get bulk text")
print(glue("Total selected papers for this cluster: {nrow(cluster_data)}"))
my_texts <- list()
for (i in c(1:min(10, nrow(cluster_data)))) {
my_texts[i] <- glue("##### {cluster_data$text[[i]]}")
}
my_texts <- paste(my_texts, collapse = " ")
my_texts <- substr(my_texts, 1, (3500 * 4))
# Get the topic of the cluster
print("Get cluster topic")
prompt_desc <- prompt_cluster_description(
topic = MAIN_TOPIC,
topic_description = MAIN_TOPIC_DESCRIPTION,
cluster_text = my_texts
)
cluster_completed <- FALSE
while (!cluster_completed) {
tmp <- tryCatch(
{
cluster_description <- ask_llm(
system_prompt = prompt_desc$system,
user_prompt = prompt_desc$user,
temperature = 0.2
)
cluster_completed <- TRUE
# print(cluster_description)
},
error = function(err) {
message(glue("Error getting topic description of cluster {cluster_code}. Trying again"))
message(err)
}
)
}
rcs_merged$description[which(rcs_merged$cluster_code == cluster_code)] <- cluster_description
# Get the name of the cluster
print("Get cluster name")
cluster_completed <- FALSE
while (!cluster_completed) {
tmp <- tryCatch(
{
prompt <- prompt_cluster_name(
topic = MAIN_TOPIC,
topic_description = MAIN_TOPIC_DESCRIPTION,
cluster_description = cluster_description
)
cluster_name <- ask_llm(
system_prompt = prompt$system,
user_prompt = prompt$user,
max_tokens = 60,
temperature = 0.3
)
cluster_completed <- TRUE
print(cluster_name)
},
error = function(err) {
message(glue("Error getting topic name of cluster {cluster}. Trying again"))
message(err)
}
)
}
rcs_merged$name[which(rcs_merged$cluster_code == cluster_code)] <- cluster_name
}
if (level_report_iteration == 0) {
print("Compute level0 Clusters")
dataset$X_C <- dataset$level0 %>% as.character()
dataset$X_E <- dataset$global_in_degree
dataset$X_E <- dataset$Z9
}
# The following are needed but they are covered in the next block.
# The most cited article in the dataset
# The top 3 most connected per cluster
# The top 3 most cited per cluster
###################################
###################################
# Cluster description and name
###################################
for (cluster_code in list_of_cluster_codes) {
# Get this cluster tops
print("=================================================================")
print(glue("cluster: {cluster_code}"))
cluster_data <- get_cluster_data(dataset, cluster_ = cluster_code, top = this_tops)
print(cluster_data$X_C)
if (COMPUTE_SUMMARIES) {
print("Computing summaries of selected articles in this cluster...")
# Summarize each of the selected papers
cluster_data <- get_papers_summary(cluster_data)
# Assign the summaries to the main dataset
print("asign summaries to main dataset")
dataset$summary[match(cluster_data$UT, dataset$UT)] <- cluster_data$summary
} else {
print("Article summaries were not requested for this cluster.")
cluster_data$text <- paste(cluster_data$TI, cluster_data$AB, sep = " ")
}
# Generate the bulk text
print("get bulk text")
print(glue("Total selected papers for this cluster: {nrow(cluster_data)}"))
my_texts <- list()
for (i in c(1:min(10, nrow(cluster_data)))) {
my_texts[i] <- glue("##### {cluster_data$text[[i]]}")
}
my_texts <- paste(my_texts, collapse = " ")
my_texts <- substr(my_texts, 1, (3500 * 4))
# Get the topic of the cluster
print("Get cluster topic")
prompt_desc <- prompt_cluster_description(
topic = MAIN_TOPIC,
topic_description = MAIN_TOPIC_DESCRIPTION,
cluster_text = my_texts
)
cluster_completed <- FALSE
while (!cluster_completed) {
tmp <- tryCatch(
{
cluster_description <- ask_llm(
system_prompt = prompt_desc$system,
user_prompt = prompt_desc$user,
temperature = 0.2
)
cluster_completed <- TRUE
# print(cluster_description)
},
error = function(err) {
message(glue("Error getting topic description of cluster {cluster_code}. Trying again"))
message(err)
}
)
}
rcs_merged$description[which(rcs_merged$cluster_code == cluster_code)] <- cluster_description
# Get the name of the cluster
print("Get cluster name")
cluster_completed <- FALSE
while (!cluster_completed) {
tmp <- tryCatch(
{
prompt <- prompt_cluster_name(
topic = MAIN_TOPIC,
topic_description = MAIN_TOPIC_DESCRIPTION,
cluster_description = cluster_description
)
cluster_name <- ask_llm(
system_prompt = prompt$system,
user_prompt = prompt$user,
max_tokens = 60,
temperature = 0.3
)
cluster_completed <- TRUE
print(cluster_name)
},
error = function(err) {
message(glue("Error getting topic name of cluster {cluster}. Trying again"))
message(err)
}
)
}
rcs_merged$name[which(rcs_merged$cluster_code == cluster_code)] <- cluster_name
}
# We do this to keep copy of the edits in case we mess it.
rcs_merged$name2 <- gsub('^.*?"', "", rcs_merged$name) %>%
gsub('".$', "", .) %>%
gsub('"', "", .)
rcs_merged$cluster_name <- rcs_merged$name2
rcs_merged$detailed_description <- rcs_merged$description
for (cluster_code in list_of_cluster_codes) {
print("=================================================================")
print(glue("cluster: {cluster_code}"))
# Get the topic of the cluster
print("Get enhanced description")
cluster_completed <- FALSE
while (!cluster_completed) {
tmp <- tryCatch(
{
prompt_enh <- prompt_cluster_description_enhanced(
topic = MAIN_TOPIC,
cluster_description = rcs_merged$detailed_description[rcs_merged$cluster_code == cluster_code]
)
print(prompt_enh$user)
cluster_description <- ask_llm(
system_prompt = prompt_enh$system,
user_prompt = prompt_enh$user,
temperature = 0.1
)
print(cluster_description)
cluster_completed <- TRUE
},
error = function(err) {
message(glue("Error getting topic enhanced description of cluster {cluster_code}. Trying again"))
message(err)
}
)
}
rcs_merged$description[which(rcs_merged$cluster_code == cluster_code)] <- cluster_description
}
# Save
write.csv(
rcs_merged %>%
select(
cluster_code, cluster_name,
documents, PY_Mean, Z9_Mean, description
),
file.path(
output_folder_path,
settings$metadata$project_folder,
settings$metadata$analysis_id,
settings$cno$clustering$algorithm,
settings$cno$thresholding$threshold,
glue("level{level_report_iteration}"),
"cluster_summary_short_tm.csv"
),
row.names = FALSE
)
write.csv(rcs_merged,
file.path(
output_folder_path,
settings$metadata$project_folder,
settings$metadata$analysis_id,
settings$cno$clustering$algorithm,
settings$cno$thresholding$threshold,
glue("level{level_report_iteration}"),
"cluster_summary_extended_tm.csv"
),
row.names = FALSE
)
library(ggplot2)
library(ggrepel)
library(stats)
library(tidyr)
library(reshape2)
heatmap_analysis_id = 'H008_Human-Aug_Innovation_Innovativeness'
settings_directive = 'heatmap_settings_H008_Human_Aug-Innovation-Innovativeness.json'
###############################################################################
# Call necessary libraries
source("zz_utils/02_libraries.R")
source("zz_utils/00_system_paths.R")
###############################################################################
# Load the directive file
settings <- RJSONIO::fromJSON(
file.path(
output_folder_path,
heatmap_analysis_id,
settings_directive
),
simplify = FALSE
)
# Save setting inputs as a df.
inputs <- lapply(settings$inputs, function(x) {data.frame(x)}) %>% rbind.fill()
###############################################################################
# Read the files
# The coordinates of all participating clusters across analysis in this heatmap
# Computed in the Heatmap colab
coords <- readr::read_csv(file.path(
output_folder_path,
heatmap_analysis_id,
"coordinates.csv"
)) %>%
select(x, y, cluster) %>%
rename(cluster_code = cluster)
# Read the RCS files
# For the heatmap the RCS is expected to have already
# The documents, PY_Mean, Z9_Mean, and the LLM_name (cluster_name)
rcs <- lapply(c(1:nrow(inputs)), \(x) {
this_df <- readr::read_csv(file.path(
output_folder_path,#settings$metadata$input_directory,
settings$inputs[[x]]$project_folder_name,
settings$inputs[[x]]$analysis_folder_name,
settings$inputs[[x]]$level_folder_name,
"cluster_summary.csv"
)) %>% mutate(
# Add the dataset name to the RCS cluster code
cluster_code = paste(inputs$display_name[[x]], cluster_code, sep = '-')
)
}) %>% rbind.fill() %>%
select(cluster_code, cluster_name, documents, PY_Mean, Z9_Mean) #Count, ave_PY, ave_Z9)
###############################################################################
###############################################################################
# Merge the datasets and create needed columns
tmp <- merge(rcs, coords, by = 'cluster_code')
tmp <- tmp %>%
separate(cluster_code,
remove = FALSE,
into = c("dataset", "local_cluster"),
sep = "-",
extra = "merge")
# When using subclusters, lets remove the trailing "---"
tmp$local_cluster <- gsub("---", "", tmp$local_cluster)
tmp <- merge(tmp,
inputs %>%
select(display_name, color, heatmap_display_order, sankey_display_order) %>%
rename(dataset = display_name),
by = 'dataset')
# The labels shown in the scatter plots
tmp$scatter_labels <- paste(tmp$local_cluster, tmp$cluster_name, sep = ':')
# Groups of clusters
km1 <- kmeans(tmp[,c("x","y")], centers = floor(sqrt(nrow(rcs))))
heatmap_analysis_id = 'H007_innovation_innovativeness'
settings_directive = 'heatmap_settings_H007_Innovation-Innovativeness.json'
###############################################################################
# Call necessary libraries
source("zz_utils/02_libraries.R")
source("zz_utils/00_system_paths.R")
###############################################################################
# Load the directive file
settings <- RJSONIO::fromJSON(
file.path(
output_folder_path,
heatmap_analysis_id,
settings_directive
),
simplify = FALSE
)
# Save setting inputs as a df.
inputs <- lapply(settings$inputs, function(x) {data.frame(x)}) %>% rbind.fill()
###############################################################################
# Read the files
# The coordinates of all participating clusters across analysis in this heatmap
# Computed in the Heatmap colab
coords <- readr::read_csv(file.path(
output_folder_path,
heatmap_analysis_id,
"coordinates.csv"
)) %>%
select(x, y, cluster) %>%
rename(cluster_code = cluster)
# Read the RCS files
# For the heatmap the RCS is expected to have already
# The documents, PY_Mean, Z9_Mean, and the LLM_name (cluster_name)
rcs <- lapply(c(1:nrow(inputs)), \(x) {
this_df <- readr::read_csv(file.path(
output_folder_path,#settings$metadata$input_directory,
settings$inputs[[x]]$project_folder_name,
settings$inputs[[x]]$analysis_folder_name,
settings$inputs[[x]]$level_folder_name,
"cluster_summary.csv"
)) %>% mutate(
# Add the dataset name to the RCS cluster code
cluster_code = paste(inputs$display_name[[x]], cluster_code, sep = '-')
)
}) %>% rbind.fill() %>%
select(cluster_code, cluster_name, documents, PY_Mean, Z9_Mean) #Count, ave_PY, ave_Z9)
###############################################################################
###############################################################################
# Merge the datasets and create needed columns
tmp <- merge(rcs, coords, by = 'cluster_code')
tmp <- tmp %>%
separate(cluster_code,
remove = FALSE,
into = c("dataset", "local_cluster"),
sep = "-",
extra = "merge")
# When using subclusters, lets remove the trailing "---"
tmp$local_cluster <- gsub("---", "", tmp$local_cluster)
tmp <- merge(tmp,
inputs %>%
select(display_name, color, heatmap_display_order, sankey_display_order) %>%
rename(dataset = display_name),
by = 'dataset')
# The labels shown in the scatter plots
tmp$scatter_labels <- paste(tmp$local_cluster, tmp$cluster_name, sep = ':')
# Groups of clusters
km1 <- kmeans(tmp[,c("x","y")], centers = floor(sqrt(nrow(rcs))))
tmp$group <- as.factor(km1$cluster)
###############################################################################
###############################################################################
# Get the scatter plots
# Aux plot function
plot_scatter_group <- function(rcs_data,
point_labels,
x_column,
y_column,
color_hex_column,
color_labels,
size_column,
x_column_label = x_column,
y_column_label = y_column,
show_tags = TRUE) {
# format the df
df <- rcs_data[, c(point_labels, x_column, y_column, color_hex_column, color_labels, size_column)]
colnames(df) <- c("point_labels", "x",    "y",       "color_hex",      "color_label", "size")
# plot
p <- ggplot(df, aes(x = x, y = y)) +
stat_ellipse(geom = "polygon",
aes(linetype = rcs_data$group),
alpha=0.07) + #0.07 OR 0.35 for colors
geom_point(aes(color = color_hex,
size = size)) +
scale_color_identity() +
xlab("") +
ylab("")
if (show_tags) {
p <- p + geom_text_repel(aes(label = gsub("---", "", point_labels)), max.overlaps = 30, size = 2)
}
p <- p + theme_bw() + theme(legend.position = "none")
p
}
#################################################
# Aux plot function
plot_scatter <- function(rcs_data,
point_labels,
x_column,
y_column,
color_hex_column,
color_labels,
size_column,
min_x,
max_x,
max_y,
x_column_label = x_column,
y_column_label = y_column,
show_tags = TRUE) {
# format the df
df <- rcs_data[, c(point_labels, x_column, y_column, color_hex_column, color_labels, size_column)]
colnames(df) <- c("point_labels", "x",    "y",       "color_hex",      "color_label", "size")
# plot
p <- ggplot(df, aes(x = x, y = y)) +
geom_point(aes(colour = color_hex,
size = size)) +
scale_color_identity() +
scale_x_continuous(limits = c(floor(min_x), ceiling(max_x))) + # = seq(2005, 2022, by = 2)) +
#scale_x_continuous(breaks = seq(2005, 2022, by = 2)) +
scale_y_continuous(limits = c(0, (round(max_y, 0) + 10))) +
xlab("") +
ylab("")
if (show_tags) {
p <- p + geom_text_repel(aes(label = gsub("---", "", point_labels)), max.overlaps = 15, size = 5)
}
p <- p + theme_bw() + theme(legend.position = "none")
p
}
#################################################
# Aux function to map values to a desired range
# Here we use it to remap the "Value" links of the Sankeys.
map_to_range <- function(x, new_min, new_max) {
# Handle edge case where all values are the same
if (max(x, na.rm = TRUE) == min(x, na.rm = TRUE)) {
return(rep(new_min, length(x)))
}
# First normalize to 0-1 range, then scale to new range
x_std <- (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
round(new_min + (new_max - new_min) * x_std, 0)
}
#################################################
# This prevents R generating unnecessary error from ggrepel
options(warn = 1)
# Global scatter by x and y MDS scalling
# i.e. Topic Model plot
tm_plot <- plot_scatter_group(tmp,
point_labels = "cluster_code",
x_column = "x",
y_column = "y",
color_hex_column = "color",
color_labels = "dataset",
size_column = "documents",
show_tags = TRUE)
tm_plot
