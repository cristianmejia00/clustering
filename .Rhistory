test1 <- dataset[grepl("patent-1", tmp_text),"UT"]
test2 <- dataset[grepl("patent-2", tmp_text),"UT"]
test3 <- dataset[grepl("patent plus", tmp_text),"UT"]
test4 <- dataset[grepl("pulmonary arterial hypertension", tmp_text),"UT"]
banned <- unique(c(test1,test2,test3,test4))
View(dataset)
banned
test <- dataset[!dataset$UT %in% banned,]
table(test$X_C)
table(test$level0)
table(test$level1)
table(test$subcluster_label1)
save(banned, file = 'banned.rdata')
library(readr)
dataset <- read_delim("C:/Users/crist/Desktop/a48195-patents-Lou/mission.facet.all.tsv",
delim = "\t", escape_double = FALSE,
trim_ws = TRUE)
View(dataset)
load("~/GitHub/clustering/banned.rdata")
table(banned %in% dataset$UT)
cls <- c(1,3,5,8,10,12,13,15,16,18,20,24,26,29,34,36,37,38,39,40,41,44,48,49,52,53,54,56,57,58)
selected <- dataset[dataset$"_C" %in% cls,]
nrow(selected) / nrow(dataset)
selected$'_N' <- NULL
selected$'_C' <- NULL
selected$'_D' <- NULL
selected$'_E' <- NULL
selected$'_F' <- NULL
selected$'_Y' <- NULL
load('banned.rdata')
selected <- selected[!(selected$UT %in% banned),]
write.table(selected, file='selected.txt', sep = '\t', row.names = FALSE)
#################################################
# Format and Save:
selected$'_N' <- NULL
selected$'_C' <- NULL
selected$'_D' <- NULL
selected$'_E' <- NULL
selected$'_F' <- NULL
selected$'_Y' <- NULL
write.table(selected, file='selected.txt', sep = '\t', row.names = FALSE)
###########################################################################################
# OPTIONS
###########################################################################################
## Select the input folders:
dataset_folder <- choose.dir()
###########################################################################################
# RUN FROM HERE
###########################################################################################
## Libraries
source("04_utils/02_libraries.R")
source(file.path(getwd(), "04_utils", "read_from_fukan_function.R"))
## Path to `/inputs`
# Here 'input' refer to the inputs for clustering.
# From the pov of this very code, this is actually the output folder. Where the files generated by this code will be placed.
bibliometrics_folder <- "C:\\Users\\crist\\OneDrive\\Documentos\\03-bibliometrics"
dir.create(file.path(bibliometrics_folder, dataset_metadata$query_id), showWarnings = FALSE)
## Read files
dataset <- read_from_fukan_2(dataset_folder)
orphans <- read_from_fukan_2(dataset_folder, what = "orphans")
## Read files
network_folder <- choose.dir()
network <- fread(paste(network_folder, "\\mission.pairs.tsv", sep = ""), sep = '\t')
# Get the network object
gd <- graph_from_data_frame(network, directed = TRUE)
gi <- graph_from_data_frame(network, directed = FALSE)
# Get degrees
gi_degree <- degree(gi, mode = 'all')
gi_degree_in <- degree(gi, mode = 'in')
gi_degree_out <- degree(gi, mode = 'out')
V(gi)$name
dfi <- data.frame('X_N' = V(gi)$name,
'all' = gi_degree,
'in' = gi_degree_in,
'out' = gi_degree_out)
View(dfi)
View(dataset)
View(dataset)
dfi <- data.frame('X_N' = as.numeric(V(gi)$name),
'all' = gi_degree,
'in' = gi_degree_in,
'out' = gi_degree_out)
dfi_extra <- merge(dfi, dataset[,'X_N', 'X_C', 'X_E', 'X_D'], by = 'X_N')
dfi_extra <- merge(dfi, dataset[,c('X_N', 'X_C', 'X_E', 'X_D')], by = 'X_N')
View(dfi_extra)
View(dfi_extra)
dfi_extra$diff <- dfi_extra$all - dfi_extra$X_D
View(dfi_extra)
dfi_extra <- merge(dfi, dataset[,c('X_N', 'X_C', 'X_E', 'X_D', 'TC', 'Z9', 'UT')], by = 'X_N')
dfi_extra$diff <- dfi_extra$all - dfi_extra$X_D
dfi_extra <- merge(dfi, dataset[,c('X_N', 'X_C', 'X_E', 'X_D', 'TC', 'Z9', 'UT')], by = 'X_N')
colnames(dataset)
dfi_extra <- merge(dfi, dataset[,c('X_N', 'X_C', 'X_E', 'X_D', 'Z9', 'UT')], by = 'X_N')
dfi_extra$diff <- dfi_extra$all - dfi_extra$X_D
View(dfi_extra)
dataset$X_N[dataset$X_C == cluster]
cluster <- 1
dataset$X_N[dataset$X_C == cluster]
dataset$X_N[dataset$X_C == cluster] %>% length
cluster <- 2
dataset$X_N[dataset$X_C == cluster] %>% length
cluster <- 1
papers_in_cluster <- dataset$X_N[dataset$X_C == cluster]
node_ids <- which(as.numeric(V(gi)$name) %in% papers_in_cluster)
gc <- subgraph(gi, node_ids)
is_directed(gc)
gc_degree <- degree(gc, mode = 'all')
gci <- data.frame('X_N' = as.numeric(V(gc)$name),
'all' = gc_degree)
View(gci)
dfi_extra <- merge(dfi_extra, gci, by = 'X_N', all.x = TRUE, all.y = TRUE)
View(dfi_extra)
# Get degrees
gd_degree <- degree(gd, mode = 'all')
gd_degree_in <- degree(gd, mode = 'in')
gd_degree_out <- degree(gd, mode = 'out')
dfd <- data.frame('X_N' = as.numeric(V(gd)$name),
'all' = gd_degree,
'in' = gd_degree_in,
'out' = gd_degree_out)
dfd_extra <- merge(dfd, dataset[,c('X_N', 'X_C', 'X_E', 'X_D', 'Z9', 'UT')], by = 'X_N')
dfd_extra$diff <- dfd_extra$all - dfd_extra$X_D
cluster <- 1
papers_in_cluster <- dataset$X_N[dataset$X_C == cluster]
node_ids <- which(as.numeric(V(gd)$name) %in% papers_in_cluster)
gc <- subgraph(gd, node_ids)
is_directed(gc)
gc_degree <- degree(gc, mode = 'all')
gc_degree_in <- degree(gc, mode = 'in')
gc_degree_out <- degree(gc, mode = 'out')
gc_degree <- degree(gc, mode = 'all')
gc_degree_in <- degree(gc, mode = 'in')
gc_degree_out <- degree(gc, mode = 'out')
gcd <- data.frame('X_N' = as.numeric(V(gc)$name),
'gc_all' = gc_degree,
'gc_in' = gc_degree_in,
'gc_out' = gc_degree_out)
dfd_extra <- merge(dfd_extra, gcd, by = 'X_N', all.x = TRUE, all.y = TRUE)
View(dfd_extra)
View(dfi_extra)
library(readr)
lou <- read_delim("C:/Users/crist/Desktop/a48200-selected-LOU/mission.facet.all.tsv",
delim = "\t", escape_double = FALSE,
trim_ws = TRUE)
View(lou)
library(readr)
neu <- read_delim("C:/Users/crist/Desktop/a48203-selected-NEU/mission.facet.all.tsv",
delim = "\t", escape_double = FALSE,
trim_ws = TRUE)
View(neu)
lou$TI[lou$`_N` == 1]
neu$TI[neu$`_N` == 1]
lou$TI[lou$`_N` == 1] == neu$TI[neu$`_N` == 1]
test <- unique(lou$`_N`, neu$`_N`)
length(test) == nrow(lou) == nrow(neu)
length(test) == nrow(lou)
test <- lapply(lou$`_N`, function(x) {
lou$TI[lou$`_N` == x] == neu$TI[neu$`_N` == x]
})
all(test)
test
all(unlist(test))
###########################################################################################
# OPTIONS
###########################################################################################
## Select the input folders:
dataset_folder <- choose.dir()
###########################################################################################
# OPTIONS
###########################################################################################
## Select the input folders:
dataset_folder <- choose.dir()
###########################################################################################
# OPTIONS
###########################################################################################
## Select the input folders:
dataset_folder <- choose.dir()
# "Properly formatted" means to remove unnecessary columns, check column types, and check column headers.
base::file.choose()
###########################################################################################
# OPTIONS
###########################################################################################
## Select the input folders:
dataset_folder <- choose.dir()
###########################################################################################
# RUN FROM HERE
###########################################################################################
## Libraries
source("04_utils/02_libraries.R")
source(file.path(getwd(), "04_utils", "read_from_fukan_function.R"))
## Path to `/inputs`
# Here 'input' refer to the inputs for clustering.
# From the pov of this very code, this is actually the output folder. Where the files generated by this code will be placed.
bibliometrics_folder <- "C:\\Users\\crist\\OneDrive\\Documentos\\03-bibliometrics"
dir.create(file.path(bibliometrics_folder, dataset_metadata$query_id), showWarnings = FALSE)
## Read files
dataset <- read_from_fukan_2(dataset_folder)
orphans <- read_from_fukan_2(dataset_folder, what = "orphans")
## Read files
network_folder <- choose.dir()
network <- fread(paste(network_folder, "\\mission.pairs.tsv", sep = ""), sep = '\t')
## Query_id
## This has de form Qxxx whith the query number from the query control file
dataset_metadata <- list("query_id" = "Q232",
"fukan_url" = "https://academic-landscape.com/analysis/44375/0/overview")
dir.create(file.path(bibliometrics_folder, dataset_metadata$query_id), showWarnings = FALSE)
## Create directories
save(dataset,orphans,dataset_metadata,
file = file.path(bibliometrics_folder, dataset_metadata$query_id, "dataset.rdata"))
##########################################################
# Load libraries
source("04_utils/02_libraries.R")
# Load input settings file
source("settings.R")
# Load data
load(file.path(
settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
"dataset.rdata"
))
# Auxiliary code to find the right number of clusters. And update the threshold.
# Get the clusters collecting 90% of papers or the top 10, whatever is the smallest number.
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum %>% plot
table(dataset$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum
table(dataset$X_C) %>% sort(decreasing = TRUE) %>%  plot()
# Load input settings file
source("settings.R")
##########################################################
# Check all documents have a cluster assigned
if (any(is.na(dataset$X_C))) {
print('CRITICAL: At least one document is missing cluster assignation!')
print('Those papers are removed')
dataset <- dataset[!is.na(dataset$X_C),]
}
##########################################################
# Document classification (Get clusters or Get topics)
if (settings$params$type_of_analysis == "citation_network") {
source(file.path(getwd(), "02_citation_network", "00_citation_network_clustering.R"))
}
# Load input settings file
source("settings.R")
##########################################################
# Document classification (Get clusters or Get topics)
if (settings$params$type_of_analysis == "citation_network") {
source(file.path(getwd(), "02_citation_network", "00_citation_network_clustering.R"))
}
if (settings$params$type_of_analysis == "topic_model") {
source(file.path(getwd(), "02_topic_model", "00_topic_model_clustering.R"))
}
# Create stats folder
dir.create(file.path(
settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder
))
source(file.path(getwd(), "03_reports", "03_general_summary.R"))
# Orphans treatment
if (settings$addons$include_orphans == "99" | settings$addons$include_orphans == "999") {
source(file.path(getwd(), "04_utils", "zz-append_orphans.R"))
}
# Add-ons
if (settings$params$type_of_analysis == "citation_network" &
exists('g1') &
(settings$addons$page_rank | settings$addons$eigen_centrality | settings$addons$closeness_centrality | settings$addons$betweeness_centrality)) {
source(file.path(getwd(), "04_utils", "zz-centrality_meassures.R"))
}
##########################################################
# save objects
if (settings$params$type_of_analysis == "topic_model") {
dataset <- myDataCorrect
}
save(dataset, file = file.path(settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder,
"dataset_clustering.rdata"))
save.image(file.path(settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder,
"environ_clustering.rdata"))
rm(list = ls())
##########################################################
# Load libraries
source("04_utils/02_libraries.R")
# Load settings from the project we are interested in
# source(file.choose())
source("settings.R")
##########################################################
# Output Folder
output_folder_reports <- file.path(settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder)
dir.create(output_folder_reports)
##########################################################
# Load data
load(file.path(
settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder,
"dataset_clustering.rdata"
))
##########################################################
# Verify the data is correctly formatted for reports
source(file.path(getwd(), "04_utils", "00_verify_data.R"))
##########################################################
# Verify the data is correctly formatted for reports
source(file.path(getwd(), "04_utils", "00_verify_data.R"))
#################################################################################
# Force to data frame object
dataset <- as.data.frame(dataset)
# Remove citation network setting for news, in case we accidentally left it here.
# If we do not remove it it will cause problem creating the heatmap keywords
if (settings$params$type_of_dataset == "news" & exists("cno")) {
rm(cno)
}
# Faceted news datasets are a special case where documents do not have any sorting metric
# And thus we add 1 to X_E
if (settings$params$type_of_dataset == "news") {
if (!exists("myDataCorrect")) {
myDataCorrect <- dataset
setnames(myDataCorrect, c('SCORE','Score'), c('score','score'), skip_absent = TRUE)
}
if (!settings$params$unit_of_analysis %in% c("topics", "topic", "clusters", "cluster")) {
myDataCorrect$cluster_code <- myDataCorrect$X_C
myDataCorrect$related_topics <- "" # This can be added with the neighbors of the network
if ('score' %in% colnames(myDataCorrect)) {
if (!'Z9' %in% colnames(myDataCorrect)) {
myDataCorrect$Z9 <- myDataCorrect$score
} else {
myDataCorrect$Z9 <- 1
}
if (!'X_E' %in% colnames(myDataCorrect)) {
myDataCorrect$X_E <- myDataCorrect$score
} else {
myDataCorrect$X_E <- 1
}
}
}
}
# Preparation for news
if (settings$params$type_of_dataset == "news") {
if (all(dataset$UT == myDataCorrect$UT)) {
print("append cols to dataset")
if (all((!c("X_C", "cluster_code", "X_E", "related_topics") %in% colnames(dataset))) &
all(c("X_C", "cluster_code", "X_E", "related_topics") %in% colnames(myDataCorrect))) {
dataset <- cbind(dataset, myDataCorrect[, c("X_C", "cluster_code", "X_E", "related_topics")])
dataset$level0 <- dataset$X_C
} else {
print("colnames of dataset seems to be OK.")
dataset$level0 <- dataset$X_C
}
} else {
print("column mismatch between dataset and myDataCorrect")
}
}
################################################################################
# Change column names
setnames(dataset, c("_N", "_E"), c("X_N", "X_E"), skip_absent = TRUE)
# Available columns at this point
available_columns <- colnames(dataset)
#################################################################################
# Append necessary columns when missing
# Critical
if (!("TI" %in% available_columns)) {
print("ERROR: NO TITLE")
}
if (!("AB" %in% available_columns)) {
print("ERROR: NO ABSTRACT")
}
if (!("X_C" %in% available_columns)) {
print("ERROR: NO CLUSTER")
}
#################################################################################
# Solvable
if (!("X_E" %in% available_columns)) {
if ("Z9" %in% available_columns) {
dataset$X_E <- dataset$Z9
}
}
if (!("PY" %in% available_columns)) {
dataset$PY <- 2000
}
if (!("DT" %in% available_columns)) {
dataset$DT <- "Article"
}
if (!("Z9" %in% available_columns)) {
dataset$Z9 <- 1
}
if (!("X_N" %in% available_columns)) {
dataset$X_N <- c(1:nrow(dataset))
}
if (!("UT" %in% available_columns)) {
dataset$UT <- dataset$X_N
}
if (!("DE" %in% available_columns)) {
dataset$DE <- dataset$TI
}
if (!("ID" %in% available_columns)) {
dataset$ID <- dataset$TI
}
#################################################################################
# Optional
if (!("WC" %in% available_columns)) {
print("warning: no WC")
}
if (!("AU" %in% available_columns)) {
print("warning: no AU")
}
if (!("DI" %in% available_columns)) {
print("warning: no DI")
}
if (!("SO" %in% available_columns)) {
print("warning: no SO")
}
if (!("C1" %in% available_columns)) {
print("warning: no C1")
}
# From bibliometrics standpoint, these papers are already there. Hence we treat papers with a future
# year as if they were published this year.
this_year <- format(Sys.Date(), "%Y") %>% as.numeric()
future_year_papers <- sum(dataset$PY > this_year)
# From bibliometrics standpoint, these papers are already there. Hence we treat papers with a future
# year as if they were published this year.
dataset$PY
# From bibliometrics standpoint, these papers are already there. Hence we treat papers with a future
# year as if they were published this year.
dataset$PY[is.na(dataset$PY)] <- 2000
dataset$PY
future_year_papers <- sum(dataset$PY > this_year)
if (future_year_papers > 0) {
print(glue('we found {future_year_papers} that will be published next year, and we treat them as published this year.'))
dataset$PY[dataset$PY > this_year] <- this_year
}
# Load utils
source("04_utils/zz_auxiliary_functions.R")
# Get countries column for news (In Factiva this is the RE regions column)
if (settings$params$type_of_dataset == "news") {
if ("C1" %in% available_columns) {
dataset$Countries <- dataset$C1
}
}
# Add Countries column
if (!("Countries") %in% available_columns) {
if ("C1" %in% available_columns) {
dataset$Countries <- getCountries(dataset$C1)
dataset$IsoCountries <- as.character(getIsoCountries(dataset$Countries))
dataset$IsoCountries <- gsub("NA; |; NA$", "", dataset$IsoCountries)
dataset$IsoCountries <- gsub("; NA", "", dataset$IsoCountries)
print("Countries column has been added")
}
}
# Add institutions column
if (!("Institutions") %in% available_columns) {
if (settings$params$type_of_dataset == "news") {
if ("ID" %in% available_columns) {
dataset$Institutions <- as.character(getInstitutions(dataset$ID))
dataset$Institutions <- gsub("NA", "", dataset$Institutions)
}
}
if (settings$params$type_of_dataset == "papers") {
if ("C1" %in% available_columns) {
dataset$Institutions <- as.character(getInstitutions(dataset$C1))
}
}
}
##########################################################################
# Format classes
dataset$X_N <- as.numeric(as.character(dataset$X_N))
dataset$X_C <- as.numeric(as.character(dataset$X_C))
dataset$PY <- as.numeric(as.character(dataset$PY))
dataset$X_E <- as.numeric(dataset$X_E)
dataset$Z9 <- as.numeric(dataset$Z9)
##########################################################################
# Clean abstract
dataset$AB <- remove_copyright_statements(dataset$AB)
dataset$AB <- remove_word_counts_line(dataset$AB)
dataset <- dataset[, !duplicated(colnames(dataset))]
if (settings$params$type_of_dataset == "news") {
myDataCorrect <- dataset
}
#dataset$X_E <- dataset$Z9
dataset$X_E[is.na(dataset$X_E)] <- 0
zz_env <- list('x01' = ls())
# Reporting clusters
source(file.path(getwd(), "02_citation_network", "01_execute_and_reports.R"))
# Save code snapshot
files_to_save <- list.files(getwd(), full.names = TRUE, recursive = TRUE)
files_to_omit <- list.files(file.path(getwd(),'renv','library'), full.names = TRUE, recursive = TRUE)
files_to_save <- setdiff(files_to_save, files_to_omit)
# Not to zip Rdata environments as they are heavy and saved separately
files_to_save <- files_to_save[!grepl('rdata$', tolower(files_to_save))]
# Zip them. This needs Rtools to work
zip(zipfile = file.path(output_folder_level, 'source_code'),
files = files_to_save)
# Save readable settings
writeLines(RJSONIO::toJSON(settings, pretty=TRUE, auto_unbox=TRUE),
file.path(output_folder_level, "settings.json"))
# Save settings object
save(settings, file = file.path(output_folder_level, "settings.rdata"))
# Save package list
session_info <- sessionInfo()
save(session_info, file = file.path(output_folder_level, "sessionInfo.rdata"))
writeLines(capture.output(sessionInfo()), file.path(output_folder_level, "sessionInfo.txt"))
# Save Global environment
save.image(file.path(output_folder_level, "environ_zz_reports.rdata"))
# Save cluster IDS
if ('fukan_original_cluster_id' %in% colnames(dataset)) {
print('Saving cluster id comparison for subclusters')
cluster_comparison <- dataset[c('X_C', 'fukan_X_C', 'fukan_original_cluster_id', 'fukan_subcluster_label')]
cluster_comparison <- cluster_comparison[!duplicated(cluster_comparison$fukan_subcluster_label),]
cluster_comparison <- cluster_comparison[order(cluster_comparison$fukan_X_C),]
write.csv(cluster_comparison, file = file.path(output_folder_level, "cluster_id_comparison.csv"), row.names = FALSE)
}
