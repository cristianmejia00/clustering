y_column,
color_hex_column,
color_labels,
size_column,
x_column_label = x_column,
y_column_label = y_column) {
# format the df
df <- rcs_data[, c(point_labels, x_column, y_column, color_hex_column, color_labels, size_column)]
colnames(df) <- c("point_labels", "x", "y", "color_hex", "color_label", "size")
df$labels <- as.character(df$point_labels)
p <- ggplot(df, aes(x = x, y = y)) +
geom_point(aes(colour = color_label,
size = size)) +
scale_color_manual(values = unique(df$color_hex)) +
xlab(x_column_label) +
ylab(y_column_label)
p <- p + geom_text_repel(aes(label = gsub("---|-0", "", labels)))
p <- p + theme_bw() + theme(legend.position = "none")
p
}
plot_scatter(tmp,
point_labels = "cluster_name",
x_column = "x",
y_column = "y",
color_hex_column = "color",
color_labels = "label",
size_column = "documents")
plot_scatter <- function(rcs_data,
point_labels,
x_column,
y_column,
color_hex_column,
color_labels,
size_column,
x_column_label = x_column,
y_column_label = y_column) {
# format the df
df <- rcs_data[, c(point_labels, x_column, y_column, color_hex_column, color_labels, size_column)]
colnames(df) <- c("point_labels", "x", "y", "color_hex", "color_label", "size")
df$labels <- as.character(df$point_labels)
p <- ggplot(df, aes(x = x, y = y)) +
geom_point(aes(colour = color_label,
size = size)) +
scale_color_manual(values = unique(df$color_hex)) +
xlab(x_column_label) +
ylab(y_column_label)
p <- p + geom_text_repel(aes(label = gsub("---|-0", "", labels)))
p <- p + theme_bw() #+ theme(legend.position = "none")
p
}
plot_scatter(tmp,
point_labels = "cluster_name",
x_column = "x",
y_column = "y",
color_hex_column = "color",
color_labels = "label",
size_column = "documents")
plot_scatter <- function(rcs_data,
point_labels,
x_column,
y_column,
color_hex_column,
color_labels,
size_column,
x_column_label = x_column,
y_column_label = y_column) {
# format the df
df <- rcs_data[, c(point_labels, x_column, y_column, color_hex_column, color_labels, size_column)]
colnames(df) <- c("point_labels", "x", "y", "color_hex", "color_label", "size")
df$labels <- as.character(df$point_labels)
p <- ggplot(df, aes(x = x, y = y)) +
geom_point(aes(colour = color_label,
size = size)) +
scale_color_manual(values = unique(df$color_hex)) +
xlab(x_column_label) +
ylab(y_column_label)
p <- p + geom_text_repel(aes(label = gsub("---|-0", "", labels)))
p <- p + theme_bw() + theme(legend.position = "none")
p
}
plot_scatter(tmp,
point_labels = "cluster_name",
x_column = "x",
y_column = "y",
color_hex_column = "color",
color_labels = "label",
size_column = "documents")
# - years x citations (or score)
# - years x size
# - size x citations
default_palette
View(rcs_tmp)
# assign hex colors
rcs_tmp$color_hex <- default_palette[as.integer(rcs_tmp$main_cluster)]
View(rcs_tmp)
# assign hex colors
rcs_tmp$main_cluster <- rcs_tmp$cluster
rcs_tmp$color_hex <- default_palette[as.integer(rcs_tmp$main_cluster)]
View(rcs_tmp)
rcs_tmp$color_hex[is.na(rcs_tmp$color_hex)] <- "#d3d3d3"
View(rcs_tmp)
View(rcs_tmp)
ttt <- dataset[1:50,]
ttt$WC2 <- strsplit(ttt$WC)
ttt$WC2 <- strsplit('; ', ttt$WC)
View(ttt)
?strsplit
ttt$WC2 <- strsplit(ttt$WC, '; ')
View(ttt)
ttt <- ttt %>%
select(UT, PY, WC, TI) %>%
unnest(WC)
library(tidyr)
ttt <- ttt %>%
select(UT, PY, WC, TI) %>%
unnest(WC)
View(ttt)
ttt <- dataset[1:50,]
ttt$WC2 <- strsplit(ttt$WC, '; ')
ttt <- ttt %>%
select(UT, PY, WC2, TI) %>%
unnest(WC2)
View(ttt)
#########################################################################
# Functions to transform a Derwent dataset to the WOS format
# So we can use it in any code created for WOS
library(plyr)
library(dplyr)
# Call necessary libraries
library(plyr)
library(Opener5)
library(data.table)
library(dplyr)
library(stringr)
library(tm)
choose.files()
file.choose()
file.choose()
# Open a window to select the directory with the files to merge
dir_path = "/Users/cristian/Library/CloudStorage/OneDrive-Personal/Documentos/00-Research projects/58 - GMO - Deals and Patents/Patents/00-Data/Update20240505"
#dir_path = "/Users/cristian/Library/CloudStorage/OneDrive-Personal/Documentos/imacros/downloads/Q299 kubota and competitors"
paths_to_files = list.files(path = dir_path, full.names= TRUE, pattern = "*.csv", recursive = TRUE)
paths_to_files = paths_to_files[grepl('.csv$', paths_to_files)]
###########################################################################################
## Path to `/inputs`
# Here 'input' refer to the inputs for clustering.
# From the pov of this very code, this is actually the output folder. Where the files generated by this code will be placed.
bibliometrics_folder <- "/Users/cristian/Library/CloudStorage/OneDrive-Personal/Documentos/03-bibliometrics" # Mac
###########################################################################################
# OPTIONS
###########################################################################################
## Query_id
## This has de form Qxxx whith the query number from the query control file
dataset_metadata <- list("query_id" = "Qgmo",
"fukan_url" = "Not apply. Directly from WOS")
#bibliometrics_folder <- "C:\\Users\\crist\\OneDrive\\Documentos\\03-bibliometrics" # Windows
dir.create(file.path(bibliometrics_folder, dataset_metadata$query_id), showWarnings = TRUE)
# Read each file and store them in a vector
# fread sometimes fails when reading the header, what to do?
list_of_all_files <- lapply(paths_to_files, function(a_path){
data1 <- readr::read_csv(a_path, skip = 1)
return(data1)})
# Verify than the files have the expected number of rows: 500. Except for a few that were the tails.
plot(unlist(sapply(list_of_all_files, nrow))) #The number of rows in each file, mostly 500.
# Create the merged dataset
dataset <- rbind.fill(list_of_all_files)
dataset <- as.data.frame(dataset)
dataset$X_N <- c(1:nrow(dataset))
# The length of the IPC to use, either 3 or 4.
IPC_digits <- 4
# ##########################################
# Helper function to obtain cut IPC code
# x is the vector of IPC in text format
# digits is the size of the cut
# unique_ipc = TRUE: Get all unique instances of IPC found in the patent, sorted from the most frequent
#              FALSE: Just let repeated values there.
IPC_list <-  function(IPC_column, digits = 4, unique_ipc = TRUE) {
members <- strsplit(IPC_column, split =" \\| ")
members <- lapply(members, function(x) x[x!=""])
members <- lapply(members, unique)
members <- lapply(members, function(x) substring(x, 1, digits))
if (unique_ipc) {
members <- lapply(members, function(x) {
temp <- table(x) %>%
sort(., decreasing = TRUE) %>%
names
return(temp)
})
}
return (lapply(members, function(x) {paste(x, collapse = " | ")}))}
# Change the contents to desired formats
# IPC of 4 digits
dataset$IPC_full <- dataset$`IPC - Current - DWPI` # Back it up
dataset$`IPC - Current - DWPI` <- IPC_list(dataset$`IPC - Current - DWPI`, IPC_digits)
# Change the pipe separator to semicolon as in WOS
piped_columns <- c(
"IPC_full",
"IPC - Current - DWPI",
"Assignee/Applicant",
"Assignee - Current US",
"Optimized Assignee",
"Ultimate Parent",
"Cited Refs - Patent",
"DWPI Family Members"
)
for (ii in piped_columns){
dataset[,ii] <- gsub(" \\| ", "; ", dataset[,ii]) %>% as.character()
}
# Change names to equivalents
new_names <- c(
UT =  "Publication Number",
TI =  "Title - DWPI",
AB = "Abstract - DWPI",
WC = "IPC - Current - DWPI",
Country = "Publication Country Code",
SO = "Ultimate Parent",
PY = "Publication Year",
Z9 = "Count of Citing Patents - DPCI",
AU = "Assignee/Applicant"
)
dataset <- dataset %>% rename(all_of(new_names))
# Prepare keywords
# Remove stopwords, numbers, and symbols from the 'text' column
dataset$DE <- dataset$TI %>%
tolower() %>%  # Convert text to lowercase
removeNumbers() %>%  # Remove numbers
removePunctuation() %>%  # Remove punctuation
removeWords(stopwords("english")) %>%  # Remove English stopwords
stripWhitespace() %>% # Remove extra whitespace
str_replace_all("\\s+", "; ") # Replace whitespace with a semicolon in the 'text_cleaned' column
# custom edits:
# Remove keywords tending to appear in patent text that are irrelevant
dataset$DE <- gsub(' eg;', '', dataset$DE) # from "e.g."
# Correct records without PY
table(dataset$PY)
table(is.na(dataset$PY))
# Corrections to Z9
dataset$Z9[is.na(dataset$Z9)] <- 0
# Remove duplicated files
dataset = dataset %>% filter(!duplicated(UT))
View(dataset)
# Check current ultimate parents
dataset$SO %>%
strsplit("; ") %>%
unlist() %>%
table() %>%
sort(decreasing = TRUE) %>%
.[1:20]
#################################################
# Save and FINISH!
#################################################
## Create directories
save(dataset,
#dataset_original,
dataset_metadata,
file = file.path(bibliometrics_folder,
dataset_metadata$query_id,
"dataset.rdata"))
##
write.csv(dataset,
row.names = FALSE,
file.path(bibliometrics_folder,
dataset_metadata$query_id,
"dataset.csv"))
library(readr)
dataset <- read_csv("~/Library/CloudStorage/OneDrive-Personal/Documentos/03-bibliometrics/Qgmo/results_full_Qgmo_2024-06-17.csv")
View(dataset)
if (min(dataset$X_C, na.rm = TRUE) == 0) {
print('Cluster -1 found. Correcting')
dataset$X_C <- dataset$X_C + 1
}
if (min(dataset$X_C, na.rm = TRUE) == 0) {
print('Cluster 0 found. Correcting')
dataset$X_C <- dataset$X_C + 1
}
if (min(dataset$X_C, na.rm = TRUE) == -1) {
print('Cluster -1 found. Correcting')
dataset$X_C <- dataset$X_C + 1
}
dataset$X_C_name <- dataset$Top_n_words
dataset$X_C_label <- dataset$Top_n_words
dataset$level0 <- dataset$X_C
dataset$level1 <- dataset$X_C
dataset$subcluster_level1 <- dataset$X_C_label
##########################################################
# Load libraries
source("04_utils/02_libraries.R")
# Load settings from the project we are interested in
# source(file.choose())
source("settings.R")
##########################################################
# Output Folder
output_folder_reports <- file.path(settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder)
dir.create(output_folder_reports)
##########################################################
# Load data
load(file.path(
settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder,
"dataset_clustering.rdata"
))
##########################################################
# Verify the data is correctly formatted for reports
source(file.path(getwd(), "04_utils", "00_verify_data.R"))
class(dataset$PY)
dataset$PY
#################################################################################
# Force to data frame object
dataset <- as.data.frame(dataset)
# Remove citation network setting for news, in case we accidentally left it here.
# If we do not remove it it will cause problem creating the heatmap keywords
if (settings$params$type_of_dataset == "news" & exists("cno")) {
rm(cno)
}
# Faceted news datasets are a special case where documents do not have any sorting metric
# And thus we add 1 to X_E
if (settings$params$type_of_dataset == "news") {
if (!exists("myDataCorrect")) {
myDataCorrect <- dataset
setnames(myDataCorrect, c('SCORE','Score'), c('score','score'), skip_absent = TRUE)
}
if (!settings$params$unit_of_analysis %in% c("topics", "topic", "clusters", "cluster")) {
myDataCorrect$cluster_code <- myDataCorrect$X_C
myDataCorrect$related_topics <- "" # This can be added with the neighbors of the network
if ('score' %in% colnames(myDataCorrect)) {
if (!'Z9' %in% colnames(myDataCorrect)) {
myDataCorrect$Z9 <- myDataCorrect$score
} else {
myDataCorrect$Z9 <- 1
}
if (!'X_E' %in% colnames(myDataCorrect)) {
myDataCorrect$X_E <- myDataCorrect$score
} else {
myDataCorrect$X_E <- 1
}
}
}
}
# Preparation for news
if (settings$params$type_of_dataset == "news") {
if (all(dataset$UT == myDataCorrect$UT)) {
print("append cols to dataset")
if (all((!c("X_C", "cluster_code", "X_E", "related_topics") %in% colnames(dataset))) &
all(c("X_C", "cluster_code", "X_E", "related_topics") %in% colnames(myDataCorrect))) {
dataset <- cbind(dataset, myDataCorrect[, c("X_C", "cluster_code", "X_E", "related_topics")])
dataset$level0 <- dataset$X_C
} else {
print("colnames of dataset seems to be OK.")
dataset$level0 <- dataset$X_C
}
} else {
print("column mismatch between dataset and myDataCorrect")
}
}
################################################################################
# Change column names
setnames(dataset, c("_N", "_E"), c("X_N", "X_E"), skip_absent = TRUE)
# Available columns at this point
available_columns <- colnames(dataset)
#################################################################################
# Append necessary columns when missing
# Critical
if (!("TI" %in% available_columns)) {
print("ERROR: NO TITLE")
}
if (!("AB" %in% available_columns)) {
print("ERROR: NO ABSTRACT")
}
if (!("X_C" %in% available_columns)) {
print("ERROR: NO CLUSTER")
}
#################################################################################
# Solvable
if (!("X_E" %in% available_columns)) {
if ("Z9" %in% available_columns) {
dataset$X_E <- dataset$Z9
}
}
if (!("PY" %in% available_columns)) {
dataset$PY <- settings$rp$most_recent_year
}
if (!("DT" %in% available_columns)) {
dataset$DT <- "Article"
}
if (!("Z9" %in% available_columns)) {
dataset$Z9 <- 1
}
if (!("X_N" %in% available_columns)) {
dataset$X_N <- c(1:nrow(dataset))
}
if (!("UT" %in% available_columns)) {
dataset$UT <- dataset$X_N
}
get_keywords_split <- function(a_column) {
text<- a_column %>%
tolower() %>%  # Convert text to lowercase
removeNumbers() %>%  # Remove numbers
removePunctuation() %>%  # Remove punctuation
removeWords(stopwords("english")) %>%  # Remove English stopwords
stripWhitespace() %>% # Remove extra whitespace
str_replace_all("\\s+", "; ")
return(text)
}
if (!("DE" %in% available_columns)) {
dataset$DE <- get_keywords_split(dataset$TI)
}
if (!("ID" %in% available_columns)) {
dataset$ID <- get_keywords_split(dataset$TI)
}
#################################################################################
# Optional
if (!("WC" %in% available_columns)) {
print("warning: no WC")
}
if (!("AU" %in% available_columns)) {
print("warning: no AU")
}
if (!("DI" %in% available_columns)) {
print("warning: no DI")
}
if (!("SO" %in% available_columns)) {
print("warning: no SO")
}
if (!("C1" %in% available_columns)) {
print("warning: no C1")
}
# From bibliometrics standpoint, these papers are already there. Hence we treat papers with a future
# year as if they were published this year.
this_year <- format(Sys.Date(), "%Y") %>% as.numeric()
future_year_papers <- sum(dataset$PY > this_year)
table(is.na(dataset$PY))
dataset$PY[is.na(dataset$PY)] <- this_year
future_year_papers <- sum(dataset$PY > this_year)
if (future_year_papers > 0) {
print(glue('we found {future_year_papers} that will be published next year, and we treat them as published this year.'))
dataset$PY[dataset$PY > this_year] <- this_year
}
# Load utils
source("04_utils/zz_auxiliary_functions.R")
# Get countries column for news (In Factiva this is the RE regions column)
if (settings$params$type_of_dataset == "news") {
if ("C1" %in% available_columns) {
dataset$Countries <- dataset$C1
}
}
# Add Countries column
if (!("Countries") %in% available_columns) {
if ("C1" %in% available_columns) {
dataset$Countries <- getCountries(dataset$C1)
dataset$IsoCountries <- as.character(getIsoCountries(dataset$Countries))
dataset$IsoCountries <- gsub("NA; |; NA$", "", dataset$IsoCountries)
dataset$IsoCountries <- gsub("; NA", "", dataset$IsoCountries)
print("Countries column has been added")
}
}
# Add institutions column
if (!("Institutions") %in% available_columns) {
if (settings$params$type_of_dataset == "news") {
if ("ID" %in% available_columns) {
dataset$Institutions <- as.character(getInstitutions(dataset$ID))
dataset$Institutions <- gsub("NA", "", dataset$Institutions)
}
}
if (settings$params$type_of_dataset == "papers") {
if ("C1" %in% available_columns) {
dataset$Institutions <- as.character(getInstitutions(dataset$C1))
}
}
}
##########################################################################
# Format classes
dataset$X_N <- as.numeric(as.character(dataset$X_N))
dataset$X_C <- as.numeric(as.character(dataset$X_C))
dataset$PY <- as.numeric(as.character(dataset$PY))
dataset$X_E <- as.numeric(dataset$X_E)
dataset$Z9 <- as.numeric(dataset$Z9)
##########################################################################
# Clean abstract
dataset$AB <- remove_copyright_statements(dataset$AB)
dataset$AB <- remove_word_counts_line(dataset$AB)
dataset <- dataset[, !duplicated(colnames(dataset))]
if (settings$params$type_of_dataset == "news") {
myDataCorrect <- dataset
}
#dataset$X_E <- dataset$Z9
dataset$X_E[is.na(dataset$X_E)] <- 0
zz_env <- list('x01' = ls())
# Reporting clusters
source(file.path(getwd(), "02_citation_network", "01_execute_and_reports.R"))
# Save code snapshot
files_to_save <- list.files(getwd(), full.names = TRUE, recursive = TRUE)
files_to_omit <- list.files(file.path(getwd(),'renv','library'), full.names = TRUE, recursive = TRUE)
files_to_save <- setdiff(files_to_save, files_to_omit)
# Not to zip Rdata environments as they are heavy and saved separately
files_to_save <- files_to_save[!grepl('rdata$', tolower(files_to_save))]
# Zip them. This needs Rtools to work
zip(zipfile = file.path(output_folder_level, 'source_code'),
files = files_to_save)
# Save readable settings
writeLines(RJSONIO::toJSON(settings, pretty=TRUE, auto_unbox=TRUE),
file.path(output_folder_level, "settings.json"))
# Save settings object
save(settings, file = file.path(output_folder_level, "settings.rdata"))
# Save package list
session_info <- sessionInfo()
save(session_info, file = file.path(output_folder_level, "sessionInfo.rdata"))
writeLines(capture.output(sessionInfo()), file.path(output_folder_level, "sessionInfo.txt"))
# Save Global environment
save.image(file.path(output_folder_level, "environ_zz_reports.rdata"))
