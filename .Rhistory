#########################################
# Creating the Document X Keywords table
ttt <- SimpleCorpus(VectorSource(enc2utf8(papersText)))
ttdm <- DocumentTermMatrix(ttt, control = list(weighting = weightTf, dictionary = temp_dict, bounds = list(global = c(5, Inf))))
term.frequency <- col_sums(ttdm) %>% sort(., decreasing = TRUE)
term.frequency <- term.frequency + 1 # To avoid inf values later
valid_names <- intersect(names(term.frequency), colnames(PHI))
term.frequency <- term.frequency[valid_names]
ttdm <- ttdm[, valid_names]
PHI <- PHI[, valid_names]
#########################################
#########################################
print("-- Creating THETA table (Documents vs Keywords)")
#########################################
# Creating the Document X Keywords table
THETA <- lapply(list_of_clusters, function(x) {
sss <- as.character(cluster_keywords_tf[[x]]$keywords)
sss <- gsub(" ", "zqzq", sss)
colindexes <- which(ttdm$dimnames$Terms %in% sss)
positions <- ttdm$j %in% colindexes
documents <- ttdm$i[positions]
scores <- ttdm$v[positions]
vectt <- rep(0, dim(ttdm)[1])
vectt[as.numeric(documents)] <- scores
return(vectt)
})
THETA <- data.frame(THETA)
THETA[is.na(THETA)] <- 0
THETA <- THETA + 0.001
PHI <- PHI + 0.001
#########################################
#########################################
print("-- Visualization parameters")
theta <- THETA / row_sums(THETA, na.rm = TRUE) # Doc x Topic, document topic matrix
phi <- PHI / row_sums(PHI, na.rm = TRUE) # Topic x Keywords
View(phi)
#########################################
# The vocabulary to be used in the visualization
# Here we get the stemed text that is in PHI columns
my_vocab <- gsub("zqzq", " ", colnames(PHI))
# To show natural text, use this. To show stem comment and do not run this.
my_vocab <- from_stem_to_raw[my_vocab] %>% unname()
my_vocab <- gsub("^a |^the ", "", my_vocab)
my_vocab <- gsub("^sustain$", "sustainability", my_vocab)
my_vocab <- gsub("^by product$", "production", my_vocab)
my_vocab <- gsub("^did models$", "models", my_vocab)
my_vocab <- gsub("[[:punct:]]", "", my_vocab) %>% trimws()
if (any(is.na(my_vocab))) {
my_vocab[is.na(my_vocab)] <- paste("--", as.character(c(1:length(which(is.na(my_vocab))))), sep = "")
}
#########################################
#########################################
print("-- Creating visualization")
#########################################
# Use TSNE for multidimensional scalling when we have too many clusters as levels 1 or above
# For level 0 we use the default scaling
if (level_report <= 4) {
myMethod <- if (level_report == 0 | nrow(myDataCorrect_SAMPLE2) <= 1500) {
function(x) LDAvis::jsPCA(x)
} else {
function(x) Rtsne(as.matrix(x), perplexity = floor((nrow(as.matrix(x)) - 1) / 3), dims = 2)$Y
}
myScale <- if (level_report == 0 | nrow(myDataCorrect_SAMPLE2) <= 1500) {
100
} else {
1000
}
# create the JSON object to feed the visualization
json <- createJSON_cnet(
phi = phi,
theta = theta,
doc.length = row_sums(ttdm)[as.numeric(rownames(theta))],
vocab = my_vocab,
term.frequency = col_sums(ttdm),
mds.method = myMethod,
plot.opts = list(xlab = "", ylab = ""),
reorder.topics = FALSE
)
# Manually change values in the JSON.
# I also could for instance, change the "createJSON" function instead.
ttt <- RJSONIO::fromJSON(json)
xxx <- table(myDataCorrect_SAMPLE$"X_C") %>% unname()
xxx <- xxx / sum(xxx, na.rm = TRUE)
ttt$mdsDat$Freq <- sqrt(xxx * myScale)
json <- RJSONIO::toJSON(ttt)
# Create the files
serVis(json,
out.dir = paste(output_folder_level, "\\keyword_explorer", sep = ""),
open.browser = FALSE
) # TRUE to open in the console
}
print("###################### reports/07_prob_exclu_keywords.R")
# INPUTS
K <- length(unique(myDataCorrect$X_C)) #K
term.frequency <- term.frequency
phi <- phi
my_vocab <- my_vocab
term.frequency
#########################################
#########################################
print("-- Parameters to compute Relevance")
#########################################
# Values inherited from the topic model codes.
# The following parameters are used by the next auxiliary function.
R <-  100
topic_seq <- rep(seq_len(K), each = R)
category <- paste0("Topic", topic_seq)
term.proportion <- term.frequency/sum(term.frequency)
phi <- t(phi)
lift <- phi / term.proportion
vocab <- my_vocab
# Auxiliary function.
find_relevance <- function(i) {
relevance <- i*log(phi) + (1 - i)*log(lift)
idx <- apply(relevance, 2,
function(x) order(x, decreasing = TRUE)[seq_len(R)])
# for matrices, we pick out elements by their row/column index
indices <- cbind(c(idx), topic_seq)
data.frame(Term = vocab[idx], Category = category,
logprob = round(log(phi[indices]), 4),
loglift = round(log(lift[indices]), 4),
relev = round(i*log(phi[indices]) + (1 - i)*log(lift[indices]), 4), #I added this line
stringsAsFactors = FALSE)
}
#########################################
#########################################
print("-- Compute relevance: most probable")
#########################################
relevance_vector <- find_relevance(1)
topic_names <- unique(relevance_vector$Category)
#Vector of terms in topics
ALL_Topics <- lapply(topic_names, function(x) {
topic_index <- which(relevance_vector$Category == x)
term <- unlist(relevance_vector$Term[topic_index])
relevance <- unlist(relevance_vector$relev[topic_index])
relevance <- relevance + abs(min(relevance)) + 0.01 #Linear scaling, to have positive weigths always
return (x = data.frame(term, relevance))})
names(ALL_Topics) <- sapply(1:length(ALL_Topics), function(x) paste("T", as.character(x)))
#########################################
#########################################
print("-- Compute relevance: most exclusive")
#########################################
relevance_vector <- find_relevance(0.03)
topic_names_exclu <- unique(relevance_vector$Category)
#Vector of terms in topics
ALL_Topics_exclu <- lapply(topic_names_exclu, function(x) {
topic_index <- which(relevance_vector$Category == x)
term <- unlist(relevance_vector$Term[topic_index])
relevance <- unlist(relevance_vector$relev[topic_index])
relevance <- relevance + abs(min(relevance)) + 0.01 #Linear scaling, to have positive weigths always
return (x = data.frame(term, relevance))})
names(ALL_Topics_exclu) <- sapply(1:length(ALL_Topics_exclu), function(x) paste("T", as.character(x)))
test <- ALL_Topics_exclu[[1]]
print("###################### reports/08_all_keywords_report.R")
# Auxiliar function normalizer
linMap <- function(x, from, to) {
# Shifting the vector so that min(x) == 0
x <- x - min(x)
# Scaling to the range of [0, 1]
x <- x / max(x)
# Scaling to the needed amplitude
x <- x * (to - from)
# Shifting to the needed level
x + from
}
##################################################
# Unify values
##################################################
# tf
cluster_keywords_tf_DF <- lapply(list_of_clusters, function(x) {
temp <- cluster_keywords_tf[[x]]
names(temp) <- c("term", "freq")
temp$cluster <- x
temp$type <- "tf"
temp$normalized <- linMap(temp[, 2], 1, 50)
return(temp)
})
cluster_keywords_tf_DF <- rbindlist(cluster_keywords_tf_DF)
# tfidf
cluster_keywords_tfidf_exclu_DF <- lapply(list_of_clusters, function(x) {
temp <- cluster_keywords_tfidf[[x]]
names(temp) <- c("term", "freq")
temp$cluster <- x
temp$type <- "tfidf"
temp$normalized <- linMap(temp[, 2], 1, 50)
return(temp)
})
cluster_keywords_tfidf_exclu_DF <- rbindlist(cluster_keywords_tfidf_exclu_DF)
# probable
ALL_Topics_DF <- lapply(c(1:length(ALL_Topics)), function(x) {
temp <- ALL_Topics[[x]]
temp$cluster <- x
temp$type <- "probable"
temp$normalized <- linMap(temp$relevance, 1, 100)
temp$relevance <- NULL
return(temp)
})
ALL_Topics_DF <- rbindlist(ALL_Topics_DF)
# exclusive
ALL_Topics_exclu_DF <- lapply(c(1:length(ALL_Topics)), function(x) {
temp <- ALL_Topics_exclu[[x]]
temp$cluster <- x
temp$type <- "exclusive"
temp$normalized <- linMap(temp$relevance, 1, 100)
temp$relevance <- NULL
return(temp)
})
ALL_Topics_exclu_DF <- rbindlist(ALL_Topics_exclu_DF)
# Unified report
unified_keywords <- rbind.fill(
cluster_keywords_tf_DF,
cluster_keywords_tfidf_exclu_DF,
ALL_Topics_DF,
ALL_Topics_exclu_DF
) # , top_papers_df, ngrams_per_cluster_df)
# Write report
write.csv(unified_keywords,
file = rn$PROJECTKeywords_report,
col.names = c("Term", "Freq", "Cluster", "Type", "Normalized"),
row.names = FALSE
)
# Save code snapshot
?zip
# Save code snapshot
zip(file.path(output_folder_level, 'source_code'),
list.files(getwd(), full.names = TRUE, recursive = TRUE))
# Save package list
list.files(getwd(), full.names = TRUE, recursive = TRUE)
# Save code snapshot
zip(zipfile = file.path(output_folder_level, 'source_code'),
files = list.files(getwd(), full.names = TRUE, recursive = TRUE))
# Save package list
Sys.getenv("R_ZIPCMD", "zip")
# Save code snapshot
zip(zipfile = 'code',
files = list.files(getwd(), full.names = TRUE, recursive = TRUE))
# Save code snapshot
# This needs Rtools to work
zip(zipfile = 'code',
files = list.files(getwd(), full.names = TRUE, recursive = TRUE))
# Save code snapshot
files_to_save <- list.files(getwd(), full.names = TRUE, recursive = TRUE)
# Not to zip Rdata environments as they are heavy and saved separately
files_to_save <- files_to_save[!grepl('rdata$', tolower(files_to_save))]
# Zip them. This needs Rtools to work
zip(zipfile = file.path(output_folder_level, 'source_code'),
files = files_to_save)
library()
(.packages())
(.packages()) %>% sort()
sessionInfo()
?sessionInfo
ttt <- sessionInfo()
ttt$platform
ttt$locale
library(jsonlite)
ListJSON=toJSON(ttt,pretty=TRUE,auto_unbox=TRUE)
class(ttt)
ttl <- as.list(sessionInfo())
ListJSON=toJSON(ttl,pretty=TRUE,auto_unbox=TRUE)
class(ttl)
ttl
ttl$
ttl$running
ttl$R.version$os
writeLines(capture.output(sessionInfo()), "sessionInfo.txt")
# Save package list
sessionInfo()
# Save package list
session_info <- sessionInfo()
# Save package list
session_info <- sessionInfo()
save(session_info, file = path(output_folder_level, "sessionInfo.rdata"))
writeLines(capture.output(sessionInfo()), path(output_folder_level, "sessionInfo.txt"))
# Save package list
session_info <- sessionInfo()
save(session_info, file = file.path(output_folder_level, "sessionInfo.rdata"))
writeLines(capture.output(sessionInfo()), file.path(output_folder_level, "sessionInfo.txt"))
writeLines(capture.output(settings), file.path(output_folder_level, "settings.txt"))
library(jsonlite)
ListJSON=toJSON(settings,pretty=TRUE,auto_unbox=TRUE)
writeLines(ListJSON, file.path(output_folder_level, "settings.json"))
ListJSON=toJSON(settings,pretty=TRUE,auto_unbox=FALSE)
writeLines(ListJSON, file.path(output_folder_level, "settings.json"))
??toJSON
ListJSON=RJSONIO::toJSON(settings, pretty=TRUE, auto_unbox=TRUE)
writeLines(ListJSON, file.path(output_folder_level, "settings2.json"))
ListJSON=RJSONIO::toJSON(settings, pretty=FALSE, auto_unbox=TRUE)
writeLines(ListJSON, file.path(output_folder_level, "settings2.json"))
##########################################################
# Select root directory
# It should be the directory where this code (00_general_parameters.R) is placed.
# setwd("/var/container/MAIN TOPIC-CLUSTERING") #Linux
# setwd(choose.dir()) #Windows
getwd()
##########################################################
# Load libraries
source("04_utils/02_libraries.R")
# Load settings from the project we are interested in
# source(file.choose())
source("settings.R")
##########################################################
# Load data
load(file.path(
settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder,
"dataset_clustering.rdata"
))
##########################################################
# Output Folder
output_folder_reports <- file.path(settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder)
dir.create(output_folder_reports)
##########################################################
# Verify the data is correctly formatted for reports
source(file.path(getwd(), "04_utils", "00_verify_data.R"))
zz_env <- list('x01' = ls())
# Reporting clusters PAPERS
if (settings$params$type_of_dataset == "papers") {
source(file.path(getwd(), "02_citation_network", "01_execute_and_reports.R"))
}
setdiff(ls(),zz_env$x05)
unlist(setdiff(ls(),zz_env$x05))
c(setdiff(ls(),zz_env$x05))
as.character(setdiff(ls(),zz_env$x05))
rm(as.character(setdiff(ls(),zz_env$x05)))
class(as.character(setdiff(ls(),zz_env$x05))
class(as.character(setdiff(ls(),zz_env$x05)))
class(as.character(setdiff(ls(),zz_env$x05)))
rm(list=as.character(setdiff(ls(),zz_env$x05)))
##########################################################
# Load libraries
source("04_utils/02_libraries.R")
# Load settings from the project we are interested in
# source(file.choose())
source("settings.R")
##########################################################
# Load data
load(file.path(
settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder,
"dataset_clustering.rdata"
))
##########################################################
# Output Folder
output_folder_reports <- file.path(settings$analysis_metadata$bibliometrics_folder,
settings$analysis_metadata$project_folder,
settings$analysis_metadata$analysis_folder)
dir.create(output_folder_reports)
##########################################################
# Verify the data is correctly formatted for reports
source(file.path(getwd(), "04_utils", "00_verify_data.R"))
zz_env <- list('x01' = ls())
# Reporting clusters PAPERS
if (settings$params$type_of_dataset == "papers") {
source(file.path(getwd(), "02_citation_network", "01_execute_and_reports.R"))
}
# Reporting clusters NEWS
if (settings$params$type_of_dataset == "news") {
output_folder <- output_folder_reports
source(file.path(getwd(), "02_topic_model", "01_1_execute_and_reports.R"))
}
# Dataset merged RCS
source(file.path(getwd(), "03_reports", "15_rcs_merged.R"))
# figures
# Save PNG figures. Normal raster figures for easy navigation in PC.
extension <- 'png'
subfolder_dataset <- "charts_dataset"
subfolder_clusters <- "charts_clusters"
source(file.path(getwd(), "zz-charts_dataset.R"))
source(file.path(getwd(), "zz-charts_clusters_stats1.R"))
source(file.path(getwd(), "zz-charts_clusters_stats2.R"))
source(file.path(getwd(), "zz-charts_clusters_scatterplots.R"))
source(file.path(getwd(), "zz-charts_trends_and_clustered_bars.R"))
# Save PNG figures. Needed for notebook.
extension <- 'svg'
subfolder_dataset <- "index_files/charts"
subfolder_clusters <- "index_files/charts"
source(file.path(getwd(), "zz-charts_dataset.R"))
source(file.path(getwd(), "zz-charts_clusters_stats1.R"))
source(file.path(getwd(), "zz-charts_clusters_stats2.R"))
source(file.path(getwd(), "zz-charts_clusters_scatterplots.R"))
source(file.path(getwd(), "zz-charts_trends_and_clustered_bars.R"))
# Save code snapshot
files_to_save <- list.files(getwd(), full.names = TRUE, recursive = TRUE)
# Not to zip Rdata environments as they are heavy and saved separately
files_to_save <- files_to_save[!grepl('rdata$', tolower(files_to_save))]
# Zip them. This needs Rtools to work
zip(zipfile = file.path(output_folder_level, 'source_code'),
files = files_to_save)
# Save readable settings
writeLines(RJSONIO::toJSON(settings, pretty=TRUE, auto_unbox=TRUE),
file.path(output_folder_level, "settings.json"))
# Save package list
session_info <- sessionInfo()
save(session_info, file = file.path(output_folder_level, "sessionInfo.rdata"))
writeLines(capture.output(sessionInfo()), file.path(output_folder_level, "sessionInfo.txt"))
# Save Global environment
save.image(file.path(output_folder_level, "environ_zz_reports.rdata"))
# transforms from a dataset to .
library(RefManageR)
library(tools)
library(stringr)
# Inputs
#dataset <- dataset[!duplicated(dataset$UT),]
dataset <- dataset
file_name <- file.path(output_folder_level, 'index_files', 'bibliography.bib')
# Check columns
if (!all(c("TI", "AU", "PY", "SO", "VL", "IS", "BP", "DI") %in% colnames(dataset))) {
stop('Necessary column for .bib is missing.')
}
# Utils
#' @description
#' Save all the articles in a dataset as a reference file `.bib`.
#' .bib files can be used in any reference manager like Zotero, and are needed for Latex and Markdown articles.
#' @param a_data_frame DATAFRAME. The dataset. (needed columns: TI, AU, PY, SO, VL, IS, BP, EP, DI, citation_key)
#' @param file_name STRING. The name of the output file including the extension `.bib`
write_refs_from_df <- function (a_data_frame, file_name = "references.bib") {
bib_docs <- lapply(c(1:nrow(a_data_frame)), function(x) {
roww <- a_data_frame[x,]
bib <- c(bibtype = "article",
key = roww$citation_key,
volume = roww$VL %>% gsub("V","",.),
number = roww$IS,
pages = roww$BP %>% gsub("P|U","",.),
title = str_to_sentence(roww$TI),
author = toTitleCase(roww$AU) %>% gsub(";", ". and", .) %>% paste(., ".", sep = ""),
journal = roww$SO %>% tolower() %>% toTitleCase(),
year = if(is.na(roww$PY)) {1800} else {roww$PY},
doi = roww$DI)
return(bib)
}) %>% as.BibEntry()
WriteBib(bib_docs, file = file_name)
}
# Create the keys for citation analysis
citation_keys <- sapply(1:nrow(dataset), \(x) {
if (dataset$AU[x] != '') {
first_au <- gsub(',|;', '', dataset$AU[x]) %>% strsplit(' ') %>% unlist() %>% tolower() %>% .[[1]]
} else {
first_au <- 'anon'
}
first_kwd <- tolower(dataset$TI[x]) %>% gsub('^the |^a |^an ', '', .) %>% gsub(' of | the | a | an | from | to | in | on ', ' ', .) %>% strsplit(' ') %>% unlist() %>% .[c(1:2)] %>% gsub('[[:punct:]]','',.) %>% paste(collapse = '-')
tmp <- paste(first_au, as.character(dataset$PY[x]), first_kwd, sep = '-')
tmp <- gsub('%|:|,|"|;', '', tmp)
if (length(tmp) > 1) {
print(x)
}
return(tmp)
})
citation_keys[duplicated(citation_keys)] <- paste(citation_keys[duplicated(citation_keys)], c(1:length(citation_keys[duplicated(citation_keys)])), sep = '')
dataset$citation_key <- citation_keys
# We get the files that have a summary.
# Because those are the only ones we reference in the generated article
dataset_bibliography <- subset(dataset, summary != '')
write_refs_from_df(dataset_bibliography, file_name = file_name)
library(logger)
install.packages('logger')
library(logger)
log_info('Loading data')
log_warn('Attention!')
t <- tempfile()
log_appender(appender_file(t))
log_info('Loading data')
log_appender()
unlink(t)
readLines(t)
t <- "log.txt"
log_appender(appender_file(t))
log_info('Loading data')
log_appender()
readLines(t)
?log_appender
t <- "log.txt"
log_appender(appender_tee(t))
log_warn('Attention!...')
log_info('Loading data ...')
log_info('New message')
readLines(t)
renv::init()
renv::init()
renv::init()
renv::snapshot()
length(files_to_save)
# Save code snapshot
files_to_save <- list.files(getwd(), full.names = TRUE, recursive = TRUE)
length(files_to_save)
?list.files
files_to_save <- list.files(getwd(), pattern = '(?!library)', full.names = TRUE, recursive = TRUE)
files_to_save <- list.files(getwd(), pattern = '^(?!library)$', full.names = TRUE, recursive = TRUE)
files_to_save <- list.files(getwd(), pattern = '^(?!(?:red|green|blue)$).*', full.names = TRUE, recursive = TRUE)
files_to_save <- list.files(getwd(), pattern = '/^(?!(?:red|green|blue)$).*/', full.names = TRUE, recursive = TRUE)
files_to_save <- list.files(getwd(), pattern = '^((?!Clide).)*', full.names = TRUE, recursive = TRUE)
files_to_save <- list.files(getwd(), pattern = "^((?!Clide).)*", full.names = TRUE, recursive = TRUE)
files_to_save <- list.files(getwd(), pattern = "^[a-lr]", full.names = TRUE, recursive = TRUE)
files_to_save <- list.files(getwd(), pattern = "s/Bob//", full.names = TRUE, recursive = TRUE)
files_to_save <- list.files(getwd(), pattern = "^(?:(?:red|green|blue|#.*)|(.*))$", full.names = TRUE, recursive = TRUE)
files_to_save
files_to_save <- list.files(getwd(), pattern = "^(?:(?!renv\/library).)*$", full.names = TRUE, recursive = TRUE)
files_to_save <- list.files(getwd(), pattern = "^(?:(?!renv\\/library).)*$", full.names = TRUE, recursive = TRUE)
files_to_save <- list.files(getwd(), pattern = "^(?:(?!(renv\/library)).)*$", full.names = TRUE, recursive = TRUE)
files_to_save <- list.files(getwd(), pattern = "^(?:(?!(library)).)*$", full.names = TRUE, recursive = TRUE)
files_to_save <- list.files(getwd(), pattern = "(?:(?!(library)).)*$", full.names = TRUE, recursive = TRUE)
files_to_save <- list.files(getwd(), pattern = "^(?:(?!(library)).)*$", full.names = TRUE, recursive = TRUE)
files_to_save <- list.files(getwd(), pattern = "^(?:(?:red|green|blue|#.*)|(.*))$", full.names = TRUE, recursive = TRUE)
files_to_save <- list.files(getwd(), pattern = "^(?:(?!library).)*$", full.names = TRUE, recursive = TRUE)
files_to_omit <- list.files(file.path(getwd(),'renv','library'), full.names = TRUE, recursive = TRUE)
files_to_save <- setdiff(files_to_save, files_to_omit)
length(files_to_save)
files_to_save
