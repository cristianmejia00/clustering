"Institutions" = "Asignees",
"DI" = "DOI",
"WC" = "IPC",
"DE" = "Author Keywords",
"Z9" = "Citations",
"score" = "Score",
# "sentiment" = "Sentiment score",
# "sentiment_factor" = "Sentiment",
"UT" = "Patent Number"
)
}
if (settings$params$dataset_source == "factiva") {
settings$rp$column_labels <- list(
"X_C" = "Cluster",
"TI" = "Headline",
"AB" = "Main paragraph",
"X_E" = "Score",
"PY" = "Publication Years",
"SO" = "Newspapers",
"AU" = "Factiva Types",
"Countries" = "Regions",
"Institutions" = "Entities",
# "WC" = "Categories",
"DE" = "Categories",
"ID" = "Entities",
"score" = "Score",
"sentiment" = "Sentiment score",
"sentiment_factor" = "Sentiment",
"UT" = "ID",
"issues" = "Issues",
"Keywords" = "Keywords"
)
}
# Activate stopwords
settings$stopwords <- list()
settings$stopwords$article_StopWords <- list(
"analysis", "paper", "na", "say", "will", "can", "article", "use", "press", "release",
"all", "rights", "reserved", "elsevier", "scopus", "doi", "int", "ieee", "cover", "story",
# Stemmed
"use", "structur", "result", "method", "system", "effect", "studi", "measur", "model", "show", "high",
"observ", "increas", "also", "propos", "two", "base", "investig", "properti", "process", "differ", "obtain",
"found", "chang"
)
# patent_StopWords <- c("patent", "claim", "device", "data", "module", "network", "control" ,
#                   "base","method", "methods","terminal", "information",
#                   "connect", "connects", "connection", "communication", "internet", "things", "thing")
#
settings$stopwords$news_Stopwords <- list(
"said", "country", "year", "according", "people", "work", "say", "says", "said",
"need", "one", "number", "well", "part", "end", "report", "support", "per", "cent", "percent",
"one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten", "billion", "million", "thousand",
"million", "time", "living", "make", "including", "however", "reached", "provide", "expected", "day",
"set", "important", "come", "many", "made", "way", "take", "total", "want", "com", "now", "like", "able", "get",
"order", "continue", "aim", "since", "monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday", "week",
"noted", "see", "addition", "put", "present", "month", "received", "taken",
"january", "february", "march", "april", "may", "june", "july", "august", "september", "november", "october", "december",
"timescontent", "especially", "know", "look", "give", "consider", "much", "asked", "lot", "less",
"yesterday", "tomorrow", "publish", "reprint", "yet", "ago"
)
settings$stopwords$myStopWords <- list(
settings$stopwords$article_StopWords,
settings$stopwords$news_Stopwords
) %>% unlist()
###############################################################################
# In the case of the analysis settings we must create the directory first.
ana_lysis_results_folder_path <- file.path(
settings$metadata$bibliometrics_folder,
settings$metadata$project_folder,
settings$metadata$analysis_id
)
dir.create(ana_lysis_results_folder_path)
# Save the analysis directive
settings_file_path <- file.path(
ana_lysis_results_folder_path,
paste("settings_analysis_directive_",
format(Sys.time(), "%Y-%m-%d-%H-%M"),
".json",
sep = ""
)
)
# Save readable settings
writeLines(
RJSONIO::toJSON(settings, pretty = TRUE, auto_unbox = TRUE),
settings_file_path
)
# Print to console
settings_file_path
# Load settings
source("_3_entry_analysis.R")
###############################################################################
# Load input files
dataset <- readr::read_csv(file.path(
output_folder_path,
settings$metadata$project_folder,
settings$metadata$filtered_folder,
"dataset_raw_cleaned.csv"
))
network <- readr::read_csv(file.path(
output_folder_path,
settings$metadata$project_folder,
settings$metadata$filtered_folder,
settings$cno$network_type,
"network.csv"
))
###############################################################################
###############################################################################
###############################################################################
# Aux. function to map components from the largest.
reorder_memberships <- function(membership) {
# Create a table of group sizes
group_sizes <- table(membership)
# Create a named vector for the mapping
# Sort group sizes in descending order and get original group numbers
size_order <- order(group_sizes, decreasing = TRUE)
# Create mapping from old to new membership numbers
mapping <- seq_along(group_sizes)
names(mapping) <- names(group_sizes)[size_order]
# Apply the mapping to the original membership vector
new_membership <- mapping[as.character(membership)]
return(new_membership)
}
# Component selection
# Create the network object and retain the largest component
g1 <- graph_from_data_frame(network, directed = TRUE)
ecount(g1)
vcount(g1)
# Cocitation
if (settings$cno$network_type == "cocitation") {
cci <- cocitation(g1)
mask <- colSums(cci) > 0
cci <- cci[mask, mask]
cci2 <- Matrix::Matrix(cci, sparse=TRUE)
g1 <- graph_from_adjacency_matrix(cci2,
mode='undirected',
weighted = TRUE,
diag = FALSE)
g1 <- simplify(g1)
rm(cci, cci2)
}
# Get components
components_membership <- components(g1)
components_membership$new_membership <- reorder_memberships(components_membership$membership)
components_sizes <- table(components_membership$new_membership)
components_available <- length(components_sizes)
# Get components df
components_df <- data.frame(
"node_ids" = c(1:length(components_membership$new_membership)),
"X_N" = V(g1) %>% names() %>% as.numeric(),
"component" = components_membership$new_membership
)
# Get the network based on strategy
if (settings$cno$component$strategy == "top") {
valid_nodes <- which(components_df$component <= settings$cno$component$value)
g_valid <- subgraph(g1, valid_nodes)
}
if (settings$cno$component$strategy == "component") {
valid_nodes <- which(components_df$component == settings$cno$component$value)
g_valid <- subgraph(g1, valid_nodes)
}
if (settings$cno$component$strategy == "min_vertices") {
valid_components <- components_sizes[components_sizes > settings$cno$component$value] %>%
names() %>%
as.integer()
valid_nodes <- which(components_df$component == valid_components)
g_valid <- subgraph(g1, valid_nodes)
}
if (settings$cno$component$strategy == "all") {
g_valid <- g1
}
###############################################################################
# Valid ids are those in the largest component, else are orphans
valid_vertices <- V(g_valid) %>%
names() %>%
as.numeric()
# Backups
dataset_backup <- dataset # backup
network_backup <- network
# Save orphans
orphans <- dataset %>% filter(!(X_N %in% valid_vertices))
# Order dataset to the order of nodes in the network
dataset <- dataset %>% filter(X_N %in% valid_vertices)
dataset <- dataset[match(valid_vertices, dataset$X_N), ]
# Network for this type of network and compnent
if (settings$cno$network_type == "cocitation") {
network <- igraph::as_data_frame(g_valid, what="edges")
} else {
# Filter the network to have only nodes in the selected network
network <- network[network$V1 %in% valid_vertices, ]
network <- network[network$V2 %in% valid_vertices, ]
}
###############################################################################
###############################################################################
###############################################################################
# Save the file
# From the pov of this very code, this is actually the output folder. Where the files generated by this code will be placed.
results_folder_path <- file.path(
settings$metadata$bibliometrics_folder,
settings$metadata$project_folder,
settings$metadata$analysis_id
)
# Write the dataset
write.csv(dataset,
file = file.path(results_folder_path, "dataset_comp.csv"),
row.names = FALSE
)
# Write orphans
write.csv(orphans,
file = file.path(results_folder_path, "orphans.csv"),
row.names = FALSE
)
# Write Network
write.csv(network,
file = file.path(results_folder_path, "network_comp.csv"),
row.names = FALSE
)
# Write Components
write.csv(components_df,
file = file.path(results_folder_path, "components.csv"),
row.names = FALSE
)
# The components settings and info
settings_save <- list(
components = settings$cno$component,
components_found = components_available,
components_sizes = components_sizes
)
writeLines(
RJSONIO::toJSON(settings_save,
pretty = TRUE,
auto_unbox = TRUE
),
file.path(results_folder_path, "component_settings.json")
)
rm(list = ls())
# 20180323 -> 20220526 -> 20221220
# Framework for Citation Network Analysis with Recursive Clustering, or Topic Models.
# Input.
# A dataset having:
# - Web of Science formatted file (header and columns)
# Output
# A `dataset.rdata` object
# Being the same input dataset but with the "X_C" column.
#==============================================================================
# Load settings
source("_3_entry_analysis.R")
###############################################################################
# Read input files
analysis_folder_path <- file.path(settings$metadata$bibliometrics_folder,
settings$metadata$project_folder,
settings$metadata$analysis_id)
dataset <- readr::read_csv(file.path(analysis_folder_path,
glue("dataset_comp.csv")))
orphans <- readr::read_csv(file.path(analysis_folder_path,
glue("orphans.csv")))
network <- readr::read_csv(file.path(analysis_folder_path,
glue("network_comp.csv")))
###############################################################################
###############################################################################
###############################################################################
# Document classification (Get clusters or Get topics)
if (settings$params$type_of_analysis %in% c("citation_network", "both")) {
source(file.path(getwd(), "02_citation_network", "00_citation_network_clustering.R"))
}
# Auxiliary code to find the right number of clusters. And update the threshold.
# Get the clusters collecting 90% of papers or the top 10, whatever is the smallest number.
table(dataset_minimal$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum %>% plot
table(dataset_minimal$X_C) %>% sort(decreasing = TRUE) %>% prop.table %>% cumsum
table(dataset_minimal$X_C) %>% sort(decreasing = TRUE) %>%  plot()
table(dataset_minimal$X_C)
# Update the threshold in settings file.
###############################################################################
###############################################################################
##############################################################################
# Save the file
# From the pov of this very code, this is actually the output folder. Where the files generated by this code will be placed.
results_folder_path <- file.path(settings$metadata$bibliometrics_folder,
settings$metadata$project_folder,
settings$metadata$analysis_id,
settings$cno$clustering$algorithm)
dir.create(results_folder_path, showWarnings = FALSE)
# Write the `dataset_minimal` which is created in 02_citation_network/02_louvain_Infomap.R
write.csv(dataset_minimal,
file = file.path(results_folder_path, "dataset_clustered.csv"),
row.names = FALSE)
# The components settings and info
network_description$algorithm <- settings$cno$clustering$algorithm
network_description$clusters_found <- dataset$X_C %>% unique() %>% length()
writeLines(RJSONIO::toJSON(network_description,
pretty = TRUE,
auto_unbox = TRUE),
file.path(results_folder_path, "network_settings.json"))
rm(list = ls())
# # ========================================================================
# # Create the summary
# source(file.path(getwd(), "03_reports", "03_general_summary.R"))
# # Orphans treatment
# if (settings$addons$include_orphans == "99" | settings$addons$include_orphans == "999") {
#   source(file.path(getwd(), "zz_utils", "zz-append_orphans.R"))
# }
#
# # Add-ons
# if (settings$params$type_of_analysis == "citation_network" &
#     exists('g1') &
#     (settings$addons$page_rank | settings$addons$eigen_centrality | settings$addons$closeness_centrality | settings$addons$betweeness_centrality)) {
#   source(file.path(getwd(), "zz_utils", "zz-centrality_meassures.R"))
# }
# file.path(report_path, "dataset_clustering.csv")
#==============================================================================
# Load settings
source("_3_entry_analysis.R")
###############################################################################
# Read input files
analysis_folder_path <- file.path(settings$metadata$bibliometrics_folder,
settings$metadata$project_folder,
settings$metadata$analysis_id)
network <- readr::read_csv(file.path(analysis_folder_path, "network_comp.csv"))
dataset_minimal <- readr::read_csv(file.path(analysis_folder_path,
settings$cno$clustering$algorithm,
"dataset_clustered.csv"))
# Create network object
if (!settings$cno$using_mission_pairs_from_fukan) {
print("Loading computed network")
g1 <- graph_from_data_frame(network, directed = TRUE)
} else {
# The network object is based on "mission.pairs.tsv" from Fukan's results
print("Graph from mission pairs")
g1 <- read.graph(network, format = "ncol")
}
# Verification
stopifnot(
all(as.numeric(dataset_minimal$X_N) %in% as.numeric(V(g1)$name))
)
dataset_minimal <- dataset_minimal[match(V(g1)$name, dataset_minimal$X_N),]
stopifnot(
all(as.numeric(dataset_minimal$X_N) == as.numeric(V(g1)$name))
)
###############################################################################
###############################################################################
###############################################################################
source("02_citation_network/03_recursive_clustering_WOS.R")
###############################################################################
###############################################################################
###############################################################################
# Save the file
# From the pov of this very code, this is actually the output folder. Where the files generated by this code will be placed.
results_folder_path <- file.path(settings$metadata$bibliometrics_folder,
settings$metadata$project_folder,
settings$metadata$analysis_id,
settings$cno$clustering$algorithm,
settings$cno$thresholding$threshold %>% as.character())
dir.create(results_folder_path, showWarnings = FALSE)
# Write the `dataset_minimal` which is created in 02_citation_network/02_louvain_Infomap.R
write.csv(dataset_minimal,
file = file.path(results_folder_path, "dataset_minimal.csv"),
row.names = FALSE)
# Write the edges counts
write.csv(edges_level1,
file = file.path(results_folder_path, "level0_edges_count.csv"),
row.names = TRUE)
settings$params$recursive_level
settings$params$recursive_level <- 1
#==============================================================================
# Load settings
source("_3_entry_analysis.R")
settings$params$recursive_level <- 1
###############################################################################
# Read input files
analysis_folder_path <- file.path(settings$metadata$bibliometrics_folder,
settings$metadata$project_folder,
settings$metadata$analysis_id)
network <- readr::read_csv(file.path(analysis_folder_path, "network_comp.csv"))
dataset_minimal <- readr::read_csv(file.path(analysis_folder_path,
settings$cno$clustering$algorithm,
"dataset_clustered.csv"))
# Create network object
if (!settings$cno$using_mission_pairs_from_fukan) {
print("Loading computed network")
g1 <- graph_from_data_frame(network, directed = TRUE)
} else {
# The network object is based on "mission.pairs.tsv" from Fukan's results
print("Graph from mission pairs")
g1 <- read.graph(network, format = "ncol")
}
# Verification
stopifnot(
all(as.numeric(dataset_minimal$X_N) %in% as.numeric(V(g1)$name))
)
dataset_minimal <- dataset_minimal[match(V(g1)$name, dataset_minimal$X_N),]
stopifnot(
all(as.numeric(dataset_minimal$X_N) == as.numeric(V(g1)$name))
)
###############################################################################
###############################################################################
###############################################################################
source("02_citation_network/03_recursive_clustering_WOS.R")
###############################################################################
###############################################################################
###############################################################################
# Save the file
# From the pov of this very code, this is actually the output folder. Where the files generated by this code will be placed.
results_folder_path <- file.path(settings$metadata$bibliometrics_folder,
settings$metadata$project_folder,
settings$metadata$analysis_id,
settings$cno$clustering$algorithm,
settings$cno$thresholding$threshold %>% as.character())
dir.create(results_folder_path, showWarnings = FALSE)
# Write the `dataset_minimal` which is created in 02_citation_network/02_louvain_Infomap.R
write.csv(dataset_minimal,
file = file.path(results_folder_path, "dataset_minimal.csv"),
row.names = FALSE)
# Write the edges counts
write.csv(edges_level1,
file = file.path(results_folder_path, "level0_edges_count.csv"),
row.names = TRUE)
if (settings$params$recursive_level >= 1) {
write.csv(edges_level2,
file = file.path(results_folder_path, "level1_edges_count.csv"),
row.names = TRUE)
}
if (settings$params$recursive_level >= 2) {
write.csv(edges_level3,
file = file.path(results_folder_path, "level2_edges_count.csv"),
row.names = TRUE)
}
if (settings$params$recursive_level >= 4) {
write.csv(edges_level4,
file = file.path(results_folder_path, "level3_edges_count.csv"),
row.names = TRUE)
}
# The components settings and info
writeLines(RJSONIO::toJSON(settings$cno$thresholding,
pretty = TRUE,
auto_unbox = TRUE),
file.path(results_folder_path, "threshold_settings.json"))
# Print info to the console:
print(dataset_minimal$level0 %>% unique() %>% sort())
print(glue("Clusters: {dataset_minimal$level0 %>% unique() %>% length()}"))
print(dataset_minimal$subcluster_label1 %>% unique() %>% sort())
print(glue("Clusters: {dataset_minimal$subcluster_label1 %>% unique() %>% length()}"))
# Clean
rm(list = ls())
# 20180323 -> 20220526
# Framework for Citation Network Analysis with Recursive Clustering, or Topic Models.
# ==============================================================================
# Load settings
source("_3_entry_analysis.R")
###############################################################################
# Load raw dataset
# Citation network assets
if (settings$params$type_of_analysis %in% c("citation_network")) {
dataset <- readr::read_csv(file.path(
settings$metadata$bibliometrics_folder,
settings$metadata$project_folder,
settings$metadata$analysis_id,
"dataset_comp.csv"
))
# Load the clustering solution once thresholded
dataset_minimal <- readr::read_csv(file.path(
settings$metadata$bibliometrics_folder,
settings$metadata$project_folder,
settings$metadata$analysis_id,
settings$cno$clustering$algorithm,
settings$cno$thresholding$threshold %>% as.character(),
"dataset_minimal.csv"
))
}
# Topic Model
if (settings$params$type_of_analysis %in% c("topic_model", "both")) {
dataset <- readr::read_csv(file.path(
settings$metadata$bibliometrics_folder,
settings$metadata$project_folder,
settings$metadata$filtered_folder,
"dataset_raw_cleaned.csv"
))
# Load the clustering solution once thresholded
dataset_minimal <- readr::read_csv(file.path(
settings$metadata$bibliometrics_folder,
settings$metadata$project_folder,
settings$metadata$analysis_id,
"dataset_minimal.csv"
))
}
# Ensure we have all the papers in the network
dataset_minimal$uuid <- dataset$uuid[match(dataset_minimal$UT, dataset$UT)]
stopifnot(all(dataset_minimal$uuid %in% dataset$uuid))
# Merge them
dataset <- merge(
dataset_minimal %>%
select(all_of(c(
"uuid",
setdiff(
colnames(dataset_minimal),
colnames(dataset)
)
))),
dataset,
by = "uuid",
all.x = TRUE,
all.y = TRUE
)
# Verify the data is correctly formatted for reports
source(file.path(getwd(), "zz_utils", "00_verify_data.R"))
zz_env <- list("x01" = ls())
###############################################################################
###############################################################################
###############################################################################
# Reporting clusters
source(file.path(
getwd(),
"02_citation_network",
"01_execute_and_reports.R"
))
