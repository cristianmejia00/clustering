{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cristianmejia00/clustering/blob/main/Topic_Models_using_BERTopic_TOPIC_MODEL_20241101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj3lYckpO2Yn"
   },
   "source": [
    "# Topic Modeling with BERTopic\n",
    "\n",
    "ðŸ”´ copied from the [Kubota Colab](https://colab.research.google.com/drive/1YsDp5_qGXGJKsEXsS8DO8CA_lqZc6EpA).  \n",
    "\n",
    "`Topic Models` are methods to automatically organize a corpus of text into topics.\n",
    "\n",
    "Topic Model process:\n",
    "1. Data preparation\n",
    "2. Tranform text to numeric vectors\n",
    "3. Multidimensionality reduction\n",
    "4. Clustering\n",
    "5. Topic analysis\n",
    "6. Cluster assignation\n",
    "\n",
    "\n",
    "This notebook uses the library `BERTopic` which is a one-stop solution for topic modeling including handy functions for plotting and analysis. However, BERTopic does not have a function to extract the X and Y coords from UMAP. If we need the coordinates then use the notebooks `Topic_Models_using_Transformers` instead. In any other situation, when a quick analysis is needed this notebook may be better.\n",
    "\n",
    "This notebook is also the one needed for the heatmap codes included in this folder.\n",
    "\n",
    "`BERTopic` is Python library that handles steps 2 to 6.\n",
    "BERT topic models use the transformer architechture to generate the embeds (i.e. the vector or numeric representation of words) and are currently the state-of-the-art method for vectorization.\n",
    "\n",
    "This notebook shows how to use it.\n",
    "\n",
    "---\n",
    "Reading:\n",
    "[Topic Modeling with Deep Learning Using Python BERTopic](https://medium.com/grabngoinfo/topic-modeling-with-deep-learning-using-python-bertopic-cf91f5676504)\n",
    "[Advanced Topic Modeling with BERTopic](https://www.pinecone.io/learn/bertopic/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlufmOowbHSF"
   },
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFQU-BC5ReYd"
   },
   "source": [
    "## Packages installation and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ckx8hOIyRZvv",
    "outputId": "20e7642f-824b-4ba1-86cc-5a97c89670c5"
   },
   "outputs": [],
   "source": [
    "!pip install bertopic[visualization]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMlZ_7DkOxGG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import uuid\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import date\n",
    "from itertools import compress\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ1BccaqRtS0"
   },
   "source": [
    "## Connect your Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6os5BGPFRxfX",
    "outputId": "41e02c25-1dc9-426e-f441-874fc600fc70"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2p9sY7sUYkU"
   },
   "outputs": [],
   "source": [
    "def find_e_keys(dictionary):\n",
    "    # List comprehension to find keys starting with 'e'\n",
    "    e_keys = [key for key in dictionary if str(key).lower().startswith('e')]\n",
    "    return e_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDm5-r5oR98q"
   },
   "source": [
    "# ðŸ”´ Input files and options\n",
    "\n",
    "Go to your Google Drive and create a folder in the root directory. We are going to save all related data in that directory.\n",
    "Upload the dataset of news into the above folder.\n",
    "- The dataset should be a `.csv` file.\n",
    "- Every row in the dataset is a document\n",
    "- It can any kind of columns. Some columns must contain the text we want to analyze. For example, a dataset of academic articles may contain a \"Title\" and/or \"Abstract\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ICq5zQbSJa8"
   },
   "outputs": [],
   "source": [
    "# The bibliometrics folder\n",
    "# Colab\n",
    "ROOT_FOLDER_PATH = \"drive/MyDrive/Bibliometrics_Drive\"\n",
    "\n",
    "# Mac\n",
    "ROOT_FOLDER_PATH = \"/Users/cristian/Library/CloudStorage/GoogleDrive-cristianmejia00@gmail.com/My Drive/Bibliometrics_Drive\"\n",
    "\n",
    "# Change to the name of the folder where the dataset is uploaded inside the above folder\n",
    "project_folder = 'Q327 TI Policy'\n",
    "\n",
    "analysis_id = 'a01_cn__f01_dc__c01_lv'\n",
    "\n",
    "# Filtered label\n",
    "settings_directive = \"settings_analysis_directive_2025-04-17-23-20.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpoR8P3O0ACf"
   },
   "outputs": [],
   "source": [
    "# Read settings\n",
    "with open(f'{ROOT_FOLDER_PATH}/{project_folder}/{analysis_id}/{settings_directive}', 'r') as file:\n",
    "    settings = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COh3zc3b2oAn"
   },
   "outputs": [],
   "source": [
    "# Input dataset\n",
    "dataset_file_path = f\"{ROOT_FOLDER_PATH}/{settings['metadata']['project_folder']}/{settings['metadata']['filtered_folder']}/dataset_raw_cleaned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biEFdxrm0WrO"
   },
   "outputs": [],
   "source": [
    "# Function to save files\n",
    "def save_as_csv(df, save_name_without_extension, with_index):\n",
    "    \"usage: `save_as_csv(dataframe, 'filename')`\"\n",
    "    df.to_csv(f\"{ROOT_FOLDER_PATH}/{save_name_without_extension}.csv\", index=with_index)\n",
    "    print(\"===\\nSaved: \", f\"{ROOT_FOLDER_PATH}/{save_name_without_extension}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ff7hQXE1dYDw"
   },
   "outputs": [],
   "source": [
    "# prompt: a function to save object to a pickle file\n",
    "def save_object_as_pickle(obj, filename):\n",
    "  \"\"\"\n",
    "  Saves an object as a pickle file.\n",
    "\n",
    "  Args:\n",
    "      obj: The object to be saved.\n",
    "      filename: The filename of the pickle file.\n",
    "  \"\"\"\n",
    "  with open(filename, \"wb\") as f:\n",
    "    pickle.dump(obj, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iljQe0_xb2FU"
   },
   "outputs": [],
   "source": [
    "# prompt: a function to load pickle object given a path\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 655
    },
    "id": "Tob-4BIaUbZ9",
    "outputId": "d48eac6d-2af8-4991-af19-5bda2386e6bc"
   },
   "outputs": [],
   "source": [
    "# Open the data file\n",
    "df = pd.read_csv(f\"{dataset_file_path}\", encoding='latin-1')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UrPiLepN1s9"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eK-U-Sl8fc-m"
   },
   "source": [
    "## PART 2: Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhwGnBjFQT4x"
   },
   "outputs": [],
   "source": [
    "# bibliometrics_folder\n",
    "# project_folder\n",
    "# project_name_suffix\n",
    "# ROOT_FOLDER_PATH = f\"drive/MyDrive/{bibliometrics_folder}\"\n",
    "\n",
    "#############################################################\n",
    "# Embeddings folder\n",
    "embeddings_folder_name = settings['tmo']['embeds_folder']\n",
    "\n",
    "# Which column has the year of the documents?\n",
    "my_year = settings['tmo']['year_column']\n",
    "\n",
    "# Number of topics. Select the number of topics to extract.\n",
    "# Choose 0, for automatic detection.\n",
    "n_topics = settings['tmo']['n_topics']\n",
    "\n",
    "# Minimum number of documents per topic\n",
    "min_topic_size = settings['tmo']['min_topic_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RYSioo2cxxC"
   },
   "outputs": [],
   "source": [
    "# Get the embeddings back.\n",
    "embeddings = load_pickle(f\"{ROOT_FOLDER_PATH}/{project_folder}/{settings['metadata']['filtered_folder']}/{embeddings_folder_name}/embeddings.pck\")\n",
    "corpus =     pd.read_csv(f\"{ROOT_FOLDER_PATH}/{project_folder}/{settings['metadata']['filtered_folder']}/{embeddings_folder_name}/corpus.csv\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "injCV_IPN26L"
   },
   "outputs": [],
   "source": [
    "# Combine embeddings\n",
    "documents = corpus.text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ve_oloYNrBLz"
   },
   "outputs": [],
   "source": [
    "# corpus['uuid'] = [uuid.uuid4() for _ in range(len(corpus.index))]\n",
    "# corpus['X_N'] = [i for i in range(1, len(corpus.index)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onVFOjBZuMmJ",
    "outputId": "4898b936-f4ea-4a7d-8448-a65466de85f2"
   },
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S7LeuG_7S4Db",
    "outputId": "4e27c106-2e0c-4630-a741-2fbd5e5512b0"
   },
   "outputs": [],
   "source": [
    "#len(embeddings) == len(documents)\n",
    "len(embeddings['embeddings']) == len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6qUvhGGcPZo"
   },
   "outputs": [],
   "source": [
    "from hdbscan.hdbscan_ import HDBSCAN\n",
    "# Execute the topic model.\n",
    "# I suggest changing the values marked with #<---\n",
    "# The others are the default values and they'll work fine in most cases.\n",
    "# This will take several minutes to finish.\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=15,\n",
    "                  n_components=5,\n",
    "                  min_dist=0.0,\n",
    "                  metric='cosine',\n",
    "                  random_state=100)\n",
    "\n",
    "if n_topics == 0:\n",
    "  # Initiate topic model with HDBScan (Automatic topic selection)\n",
    "  topic_model_params = HDBSCAN(min_cluster_size=min_topic_size,\n",
    "                               metric='euclidean',\n",
    "                               cluster_selection_method='eom',\n",
    "                               prediction_data=True)\n",
    "else:\n",
    "  # Initiate topic model with K-means (Manual topic selection)\n",
    "  topic_model_params = KMeans(n_clusters = n_topics)\n",
    "\n",
    "# Initiate BERTopic\n",
    "topic_model = BERTopic(umap_model = umap_model,\n",
    "                       hdbscan_model = topic_model_params,\n",
    "                       min_topic_size=min_topic_size,\n",
    "                       #nr_topics=15,          #<--- Footnote 1\n",
    "                       n_gram_range=(1,3),\n",
    "                       language='english',\n",
    "                       calculate_probabilities=True,\n",
    "                       verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "# Footnote 1: This controls the number of topics we want AFTER clustering.\n",
    "# Add a hashtag at the beggining to use the number of topics returned by the topic model.\n",
    "# When using HDBScan nr_topics will be obtained after orphans removal, and there is no warranty that `nr_topics < HDBScan topics`.\n",
    "# thus, with HDBScan `nr_topics` means N topics OR LESS.\n",
    "# When using KMeans nr_topics can be used to further reduce the number of topics.\n",
    "# We use the topics as returned by the topic model. So we do not need to activate it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQQsmPGzUCuB",
    "outputId": "a2b43bac-051a-4599-ad04-a36b1a575a26"
   },
   "outputs": [],
   "source": [
    "# Compute topic model\n",
    "#topics, probabilities = topic_model.fit_transform(documents, embeddings)\n",
    "topics, probabilities = topic_model.fit_transform(documents, embeddings['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "67pgsMtwsFmH",
    "outputId": "82c86e95-6be7-4832-a076-94d8d9dcd8ae"
   },
   "outputs": [],
   "source": [
    "# Get the list of topics\n",
    "# Topic = the topic number. From the largest topic.\n",
    "#         \"-1\" is the generic topic. Genericr keywords are aggegrated here.\n",
    "# Count = Documents assigned to this topic\n",
    "# Name = Top 4 words of the topic based on probability\n",
    "# Representation = The list of words representing this topic\n",
    "# Representative_Docs = Documents assigned to this topic\n",
    "tm_summary = topic_model.get_topic_info()\n",
    "tm_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9mN1tgZQupy"
   },
   "outputs": [],
   "source": [
    "# Save the topic model assets\n",
    "tm_folder_path = f'{ROOT_FOLDER_PATH}/{project_folder}/{settings[\"metadata\"][\"analysis_id\"]}'\n",
    "\n",
    "if not os.path.exists(tm_folder_path):\n",
    "  !mkdir $tm_folder_path\n",
    "\n",
    "tm_summary.to_csv(f'{tm_folder_path}/topic_model_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WuE0vkbp3fVL",
    "outputId": "aa8ebf45-cdd3-4161-9ac4-1343c91f47da"
   },
   "outputs": [],
   "source": [
    "# Number of topics found\n",
    "found_topics = max(tm_summary.Topic) + 1\n",
    "found_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KjKUcNMdwFHQ",
    "outputId": "b3549b7c-4c8a-4b82-8d87-6a46774e4694"
   },
   "outputs": [],
   "source": [
    "# Confirm all documents are assigned\n",
    "sum(tm_summary.Count) == len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bGyoQT47xY_R",
    "outputId": "9affafe5-0ff7-4edf-d9f6-3d7a7eda04eb"
   },
   "outputs": [],
   "source": [
    "# Get top 10 terms for a topic\n",
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PU99D2xhxoOY",
    "outputId": "4e90e928-9bbf-40c3-c3b5-a1a50abe82f0"
   },
   "outputs": [],
   "source": [
    "# Get the top 10 documents for a topic\n",
    "topic_model.get_representative_docs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CgYkbHN9I2T"
   },
   "outputs": [],
   "source": [
    "# Others\n",
    "\n",
    "# # Get the number of documents per topic (same as in the table above)\n",
    "# topic_model.get_topic_freq(0)\n",
    "\n",
    "# # Get the main keywords per topic\n",
    "# topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQKxUqHN9RT7",
    "outputId": "c572d73b-fc91-4c84-dc79-8f047b046aa2"
   },
   "outputs": [],
   "source": [
    "# Print the parameters used. (For reporting)\n",
    "topic_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G8hwSK98RM9-",
    "outputId": "07a40193-c69a-4356-b1b0-ac721ef78c08"
   },
   "outputs": [],
   "source": [
    "tm_params = dict(topic_model.get_params())\n",
    "for key, value in tm_params.items():\n",
    "    tm_params[key]=  str(value)\n",
    "with open(f'{tm_folder_path}/topic_model_params.json', 'w') as f:\n",
    "    json.dump(tm_params, f, ensure_ascii=False, indent=4)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5v4Wvn3ZAkZP",
    "outputId": "ea347717-a33e-40ad-ad2d-51798cbc5b5f"
   },
   "outputs": [],
   "source": [
    "# Get the topic score for each paper and its assigned topic\n",
    "topic_distr, _ = topic_model.approximate_distribution(documents, batch_size=1000)\n",
    "distributions = [distr[topic] if topic != -1 else 0 for topic, distr in zip(topics, topic_distr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6OTw5go4yVvM",
    "outputId": "7dfa090e-776f-4922-d611-325b1510ccab"
   },
   "outputs": [],
   "source": [
    "topic_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "I7E0_6bv9hBp",
    "outputId": "06faea2f-1c00-4ca7-958b-4dd2dcb10c36"
   },
   "outputs": [],
   "source": [
    "# Document information. Including the topic assignation\n",
    "dataset_clustering_results = topic_model.get_document_info(documents, df = corpus, metadata={\"Score\": distributions})\n",
    "\n",
    "# Standar format for report analysis\n",
    "dataset_clustering_results = dataset_clustering_results.drop(columns=['text'])\n",
    "dataset_clustering_results['X_E'] = dataset_clustering_results['Score']\n",
    "dataset_clustering_results['X_C'] = dataset_clustering_results['Topic'] + 1\n",
    "dataset_clustering_results['level0'] = dataset_clustering_results['Topic'] + 1\n",
    "dataset_clustering_results['cl99'] = False\n",
    "dataset_clustering_results['cl-99'] = False\n",
    "dataset_clustering_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyQDhrjyc6Oh"
   },
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "dataset_clustering_results.to_csv(f'{tm_folder_path}/dataset_minimal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1RYcsXanZvrr",
    "outputId": "d625d86f-5897-4d75-8b32-54f2dd258ad3"
   },
   "outputs": [],
   "source": [
    "# Save the topic model\n",
    "topic_model.save(f'{tm_folder_path}/topic_model_object.pck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SX1EBSOY3Pz1"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM4oE6dpT6UwjIg42kzzsWo",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "env-tm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
